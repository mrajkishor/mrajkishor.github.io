

# ğŸ“¡ Amazon Kinesis Data Streams â€“ Hands-On Tutorial for Real-Time Data Streaming

## ğŸ§  What is Amazon Kinesis Data Streams?

**Amazon Kinesis Data Streams (KDS)** is a fully managed service by AWS that lets you **collect, process, and store real-time streaming data**. It is designed for scenarios like:

- Website clickstream tracking  
- IoT sensor data collection  
- Real-time log monitoring  
- Event-driven analytics  

With Kinesis, you can build applications that **react instantly to new data**.

---

## ğŸ› ï¸ What Will We Do in This Hands-On?

In this tutorial, weâ€™ll:
1. Create a Kinesis Data Stream  
2. Create a Producer (using AWS CLI or SDK) to send data  
3. Set up a Consumer (AWS Lambda) to process data  
4. Explore Data Replay and Retention  
5. Monitor the stream with CloudWatch  

---

## ğŸ”§ Prerequisites

- AWS account with admin access  
- AWS CLI configured (`aws configure`)  
- Optional: Node.js / Python for SDK-based producer  
- Basic understanding of Lambda and IAM

---

## ğŸ“ Step 1: Create a Kinesis Data Stream

### Option 1: Via AWS Console
1. Go to **Amazon Kinesis > Data Streams**
2. Click **Create data stream**
3. Name: `MyDemoStream`
4. Choose:
   - **On-Demand** mode (auto-scales based on load)
   - Or **Provisioned** (manually define shards)
5. Click **Create Stream**

âœ… Done! Your stream is now ready to accept data.

---

## âœ‰ï¸ Step 2: Send Data to Kinesis (Producer)

You can send data using:
- AWS CLI
- AWS SDK (Python, Node.js, Java)
- Kinesis Agent (for logs)

### âœ… AWS CLI Example

```bash
aws kinesis put-record \
  --stream-name MyDemoStream \
  --partition-key user-1 \
  --data "Hello from CLI"
```

> Tip: Partition key determines which shard the data goes to.

---

### âœ… Node.js Example (Producer)

```js
const AWS = require('aws-sdk');
AWS.config.update({ region: 'us-east-1' });

const kinesis = new AWS.Kinesis();

const params = {
  Data: JSON.stringify({ user: 'user1', message: 'Hello from Node' }),
  PartitionKey: 'user-1',
  StreamName: 'MyDemoStream'
};

kinesis.putRecord(params, (err, data) => {
  if (err) console.error(err);
  else console.log("Data sent:", data);
});
```

---

## ğŸ› ï¸ Step 3: Process Data Using AWS Lambda (Consumer)

### Create a Lambda Function

1. Go to **AWS Lambda > Create function**
2. Name: `KinesisConsumer`
3. Runtime: Python / Node.js
4. Permissions: Create a new role with basic Lambda permissions

### Add Trigger

1. In the Lambda dashboard, click **â€œAdd triggerâ€**
2. Choose **Kinesis**
3. Select `MyDemoStream`
4. Set batch size (e.g., 100)
5. Enable trigger

### Sample Lambda Code (Node.js)

```js
exports.handler = async (event) => {
  for (const record of event.Records) {
    const payload = Buffer.from(record.kinesis.data, 'base64').toString('utf-8');
    console.log("Received record:", payload);
  }
  return `Processed ${event.Records.length} records.`;
};
```

> âœ… Now, every message sent to the stream is processed by this Lambda!

---

## âª Step 4: Reprocess / Replay Data

Kinesis allows replaying data from a given timestamp or sequence number.

### Replay from a Specific Timestamp (Python Boto3 Example)

```python
kinesis = boto3.client('kinesis')
shard_iterator = kinesis.get_shard_iterator(
    StreamName='MyDemoStream',
    ShardId='shardId-000000000000',
    ShardIteratorType='AT_TIMESTAMP',
    Timestamp=datetime(2024, 4, 2, 10, 0, 0)
)['ShardIterator']

records = kinesis.get_records(ShardIterator=shard_iterator, Limit=100)
for r in records['Records']:
    print(r['Data'])
```

---

## ğŸ“ˆ Step 5: Monitor Your Stream

Kinesis integrates with **CloudWatch** to track:

- IncomingBytes / Records
- GetRecords success/failure
- Iterator age (how late your consumer is)

Go to **CloudWatch > Metrics > Kinesis** to visualize and set alarms.

---

## ğŸ” Bonus: Security Best Practices

- ğŸ”’ Use **IAM policies** to control access to streams
- ğŸ” Enable **KMS encryption at rest**
- ğŸŒ Use **VPC endpoints** to keep traffic inside your private network

---

## ğŸ§¾ Summary

| Component  | Action                                    |
|------------|--------------------------------------------|
| Producer   | Pushes data to the stream                 |
| Kinesis DS | Buffers data (24hâ€“365d) and routes it     |
| Consumer   | Pulls and processes data (e.g., Lambda)   |
| Replay     | Reprocess old data                        |
| Monitoring | Via CloudWatch                            |

---

## ğŸ§  Final Thoughts

With Kinesis Data Streams, you can build **reactive, real-time applications** at scale. It gives you full control over how data is ingested, stored, and processed.

âœ… **Use Kinesis** when:
- You need event-by-event processing
- Low-latency alerting and analytics are required
- Replay, durability, and ordering are essential

