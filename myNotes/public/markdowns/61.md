# A Comprehensive Guide to K-Medoid Clustering

## Introduction

K-Medoid clustering is an unsupervised machine learning algorithm designed to partition data into clusters based on similarity. It is closely related to K-Means clustering but is more robust to noise and outliers because it selects actual data points (medoids) as cluster centers rather than using the mean of cluster points like K-Means. This feature makes K-Medoid particularly useful in scenarios where the dataset contains outliers or non-numeric attributes.

In this blog, we'll explore the concepts, steps, advantages, limitations, and practical applications of K-Medoid clustering, with a detailed implementation example.

---

## What is K-Medoid Clustering?

K-Medoid clustering groups data into \( K \) clusters by identifying representative data points, called **medoids**, that minimize the total dissimilarity between cluster members and their medoids. Unlike K-Means, K-Medoid ensures that the selected cluster center is always one of the actual data points in the dataset.

### Key Features:
- **Centroid-based Clustering:** Uses medoids instead of centroids.
- **Distance-based:** Minimizes the sum of dissimilarities between data points and their cluster medoid.
- **Robust to Outliers:** Less influenced by outliers compared to K-Means.

---

## How Does K-Medoid Work?

The algorithm typically uses **PAM (Partitioning Around Medoids)** to achieve clustering. Below are the steps:

### 1. Initialization
- Choose the number of clusters, \( K \).
- Randomly select \( K \) data points as initial medoids.

### 2. Assignment
- Assign each data point to the cluster with the closest medoid based on a distance metric (e.g., Euclidean, Manhattan).

### 3. Update
- For each cluster, identify the medoid as the data point that minimizes the total intra-cluster dissimilarity.

### 4. Iteration
- Repeat the Assignment and Update steps until:
  - Medoids remain unchanged.
  - A maximum number of iterations is reached.

---

### Objective Function
The algorithm minimizes the following objective function:
\[
J = \sum_{i=1}^{K} \sum_{x \in C_i} d(x, m_i)
\]
Where:
- \( K \): Number of clusters.
- \( x \): Data point in cluster \( C_i \).
- \( m_i \): Medoid of cluster \( C_i \).
- \( d(x, m_i) \): Distance between \( x \) and \( m_i \).

---

## Example of K-Medoid Clustering

### Problem:
Given the dataset:
\[
\text{Points: } A(2, 6), B(3, 4), C(6, 5), D(8, 7), E(6, 8), F(7, 6)
\]
Perform K-Medoid clustering with \( K = 2 \) using Euclidean distance.

### Step 1: Initialization
- Randomly select \( K = 2 \) medoids. Assume \( A(2, 6) \) and \( D(8, 7) \) are chosen.

### Step 2: Assignment
Calculate the distance of each point to the medoids and assign each point to the nearest medoid.

| Point | Distance to \( A(2, 6) \) | Distance to \( D(8, 7) \) | Cluster |
|-------|---------------------------|---------------------------|---------|
| \( A(2, 6) \) | 0.0                 | \( \sqrt{(8-2)^2 + (7-6)^2} = 6.08 \) | 1       |
| \( B(3, 4) \) | \( \sqrt{(3-2)^2 + (4-6)^2} = 2.24 \) | \( \sqrt{(8-3)^2 + (7-4)^2} = 5.83 \) | 1       |
| \( C(6, 5) \) | \( \sqrt{(6-2)^2 + (5-6)^2} = 4.12 \) | \( \sqrt{(8-6)^2 + (7-5)^2} = 2.83 \) | 2       |
| \( D(8, 7) \) | \( \sqrt{(8-2)^2 + (7-6)^2} = 6.08 \) | 0.0                             | 2       |
| \( E(6, 8) \) | \( \sqrt{(6-2)^2 + (8-6)^2} = 4.47 \) | \( \sqrt{(8-6)^2 + (8-7)^2} = 2.24 \) | 2       |
| \( F(7, 6) \) | \( \sqrt{(7-2)^2 + (6-6)^2} = 5.0 \)  | \( \sqrt{(8-7)^2 + (7-6)^2} = 1.41 \) | 2       |

**Clusters:**
- Cluster 1: \( A(2, 6), B(3, 4) \)
- Cluster 2: \( C(6, 5), D(8, 7), E(6, 8), F(7, 6) \)

### Step 3: Update
For each cluster, select the point that minimizes the total intra-cluster distance as the new medoid.

- **Cluster 1:** Candidates: \( A(2, 6), B(3, 4) \)
  - \( A: d(A, B) = 2.24 \)
  - \( B: d(B, A) = 2.24 \)

  Medoid: \( A(2, 6) \) (unchanged).

- **Cluster 2:** Candidates: \( C(6, 5), D(8, 7), E(6, 8), F(7, 6) \)
  - \( C: d(C, D) + d(C, E) + d(C, F) = 2.83 + 3.0 + 1.0 = 6.83 \)
  - \( D: d(D, C) + d(D, E) + d(D, F) = 2.83 + 2.24 + 1.41 = 6.48 \)
  - \( E: d(E, C) + d(E, D) + d(E, F) = 3.0 + 2.24 + 2.24 = 7.48 \)
  - \( F: d(F, C) + d(F, D) + d(F, E) = 1.0 + 1.41 + 2.24 = 4.65 \)

  Medoid: \( F(7, 6) \).

### Repeat Until Convergence
Continue the assignment and update steps until the medoids remain the same.

---

## Advantages of K-Medoid Clustering

1. **Robust to Outliers:** Since medoids are actual data points, the algorithm is less influenced by extreme values.
2. **Works with Dissimilarity Measures:** Can handle non-Euclidean distance metrics.
3. **More Stable:** Medoids are less affected by changes in the dataset compared to centroids.

---

## Limitations of K-Medoid Clustering

1. **Computational Complexity:** More expensive than K-Means, especially for large datasets, because it evaluates all possible medoid candidates.
2. **Initial Selection Sensitivity:** Results depend on the initial choice of medoids.
3. **Scalability Issues:** Inefficient for very large datasets compared to K-Means.

---

## Applications of K-Medoid Clustering

1. **Market Segmentation:**
   - Identify representative customer groups for personalized marketing.
2. **Healthcare:**
   - Cluster patients based on medical history for better treatment plans.
3. **Fraud Detection:**
   - Group transactions to detect anomalies.
4. **Document Classification:**
   - Group similar documents for information retrieval.

---

## Python Implementation of K-Medoid Clustering

```python
from sklearn_extra.cluster import KMedoids
import numpy as np
import matplotlib.pyplot as plt

# Sample dataset
data = np.array([[2, 6], [3, 4], [6, 5], [8, 7], [6, 8], [7, 6]])

# Apply K-Medoid clustering
kmedoids = KMedoids(n_clusters=2, random_state=0, metric='euclidean').fit(data)

# Cluster labels and medoids
labels = kmedoids.labels_
medoids = kmedoids.cluster_centers_

# Plot clusters
plt.scatter(data[:, 0], data[:, 1], c=labels, s=50, cmap='viridis')
plt.scatter(medoids[:, 0], medoids[:, 1], c='red', s=200, label='Medoids')
plt.title("K-Medoid Clustering")
plt.legend()
plt.show()
```

---

## Numerical Problems:

Here are some numerical problems based on **K-Medoid Clustering**. These problems will help you understand the process of selecting medoids, assigning clusters, and minimizing the total intra-cluster dissimilarity.

---

### **Problem 1: Basic K-Medoid Clustering**

#### Problem:
You are given the following data points in a 2D space:
\[
A(1, 2), B(2, 3), C(4, 4), D(5, 6), E(8, 8)
\]
Use \( K = 2 \) clusters and follow these steps:
1. Choose \( A(1, 2) \) and \( E(8, 8) \) as the initial medoids.
2. Assign each point to the nearest medoid based on **Euclidean distance**.
3. Compute the total intra-cluster dissimilarity.
4. Swap the medoid \( E(8, 8) \) with \( D(5, 6) \) and calculate the new total dissimilarity. Compare the results.

---

### **Problem 2: Swap-Based Medoid Optimization**

#### Problem:
Given the dataset:
\[
(1, 1), (2, 2), (3, 3), (8, 8), (9, 9), (10, 10)
\]
- Assume \( K = 2 \) and the initial medoids are \( (1, 1) \) and \( (8, 8) \).
- Calculate the total dissimilarity for the initial medoids.
- Swap the medoid \( (8, 8) \) with \( (9, 9) \), reassign clusters, and compute the new total dissimilarity.
- Determine if the swap improves the clustering.

---


These problems will test your understanding of medoid selection, cluster assignment, and the iterative optimization process of K-Medoid clustering. 

---

## Solutions: 

Here are the solutions to the numerical problems on **K-Medoid Clustering**:


### **Solution to Problem 1: Basic K-Medoid Clustering**

#### Problem Recap:
Data points: \( A(1, 2), B(2, 3), C(4, 4), D(5, 6), E(8, 8) \)  
Initial medoids: \( A(1, 2) \) and \( E(8, 8) \).

---

#### Step 1: Compute Distances and Assign Clusters
Use the **Euclidean distance formula**:
\[
d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
\]

| Point | Distance to \( A(1, 2) \) | Distance to \( E(8, 8) \) | Assigned Cluster |
|-------|---------------------------|---------------------------|------------------|
| \( A(1, 2) \) | 0.0                 | \( \sqrt{(8-1)^2 + (8-2)^2} = 9.22 \) | 1 |
| \( B(2, 3) \) | \( \sqrt{(2-1)^2 + (3-2)^2} = 1.41 \) | \( \sqrt{(8-2)^2 + (8-3)^2} = 7.81 \) | 1 |
| \( C(4, 4) \) | \( \sqrt{(4-1)^2 + (4-2)^2} = 3.61 \) | \( \sqrt{(8-4)^2 + (8-4)^2} = 5.66 \) | 1 |
| \( D(5, 6) \) | \( \sqrt{(5-1)^2 + (6-2)^2} = 5.66 \) | \( \sqrt{(8-5)^2 + (8-6)^2} = 3.61 \) | 2 |
| \( E(8, 8) \) | \( \sqrt{(8-1)^2 + (8-2)^2} = 9.22 \) | 0.0                             | 2 |

Clusters:
- Cluster 1: \( A(1, 2), B(2, 3), C(4, 4) \)
- Cluster 2: \( D(5, 6), E(8, 8) \)

---

#### Step 2: Compute Total Dissimilarity
**Cluster 1 Dissimilarity:**
\[
d(A, A) + d(B, A) + d(C, A) = 0 + 1.41 + 3.61 = 5.02
\]

**Cluster 2 Dissimilarity:**
\[
d(D, E) + d(E, E) = 3.61 + 0 = 3.61
\]

Total Dissimilarity = \( 5.02 + 3.61 = 8.63 \)

---

#### Step 3: Swap \( E(8, 8) \) with \( D(5, 6) \)
New medoids: \( A(1, 2), D(5, 6) \).

Reassign clusters and calculate the total dissimilarity again.

| Point | Distance to \( A(1, 2) \) | Distance to \( D(5, 6) \) | Assigned Cluster |
|-------|---------------------------|---------------------------|------------------|
| \( A(1, 2) \) | 0.0                 | \( \sqrt{(5-1)^2 + (6-2)^2} = 5.66 \) | 1 |
| \( B(2, 3) \) | \( \sqrt{(2-1)^2 + (3-2)^2} = 1.41 \) | \( \sqrt{(5-2)^2 + (6-3)^2} = 4.24 \) | 1 |
| \( C(4, 4) \) | \( \sqrt{(4-1)^2 + (4-2)^2} = 3.61 \) | \( \sqrt{(5-4)^2 + (6-4)^2} = 2.24 \) | 2 |
| \( D(5, 6) \) | \( \sqrt{(5-1)^2 + (6-2)^2} = 5.66 \) | 0.0                             | 2 |
| \( E(8, 8) \) | \( \sqrt{(8-1)^2 + (8-2)^2} = 9.22 \) | \( \sqrt{(8-5)^2 + (8-6)^2} = 3.61 \) | 2 |

**New Clusters:**
- Cluster 1: \( A(1, 2), B(2, 3) \)
- Cluster 2: \( C(4, 4), D(5, 6), E(8, 8) \)

**New Total Dissimilarity:**
\[
\text{Cluster 1: } 0 + 1.41 = 1.41
\]
\[
\text{Cluster 2: } 2.24 + 3.61 + 0 = 5.85
\]

**Total Dissimilarity = \( 1.41 + 5.85 = 7.26 \)**

Since \( 7.26 < 8.63 \), the swap improves clustering.

---

### **Solution to Problem 2: Swap-Based Medoid Optimization**

#### Problem Recap:
Points: \( (1, 1), (2, 2), (3, 3), (8, 8), (9, 9), (10, 10) \)  
Initial medoids: \( (1, 1), (8, 8) \).

---

#### Step 1: Compute Total Dissimilarity
Assign points to the nearest medoid.

| Point | Distance to \( (1, 1) \) | Distance to \( (8, 8) \) | Cluster |
|-------|--------------------------|--------------------------|---------|
| \( (1, 1) \) | 0.0                | \( \sqrt{(8-1)^2 + (8-1)^2} = 9.89 \) | 1       |
| \( (2, 2) \) | \( \sqrt{(2-1)^2 + (2-1)^2} = 1.41 \) | \( \sqrt{(8-2)^2 + (8-2)^2} = 8.49 \) | 1       |
| \( (3, 3) \) | \( \sqrt{(3-1)^2 + (3-1)^2} = 2.83 \) | \( \sqrt{(8-3)^2 + (8-3)^2} = 7.07 \) | 1       |
| \( (8, 8) \) | \( \sqrt{(8-1)^2 + (8-1)^2} = 9.89 \) | 0.0                              | 2       |
| \( (9, 9) \) | \( \sqrt{(9-1)^2 + (9-1)^2} = 11.31 \) | \( \sqrt{(9-8)^2 + (9-8)^2} = 1.41 \) | 2       |
| \( (10, 10) \) | \( \sqrt{(10-1)^2 + (10-1)^2} = 12.73 \) | \( \sqrt{(10-8)^2 + (10-8)^2} = 2.83 \) | 2       |

Clusters:
- Cluster 1: \( (1, 1), (2, 2), (3, 3) \)
- Cluster 2: \( (8, 8), (9, 9), (10, 10) \)

**Total Dissimilarity:**
\[
\text{Cluster 1: } 0 + 1.41 + 2.83 = 4.24
\]
\[
\text{Cluster 2: } 0 + 1.41 + 2.83 = 4.24
\]
**Total = \( 4.24 + 4.24 = 8.48 \)**

---

#### Step 2: Swap Medoid \( (8, 8) \) with \( (9, 9) \)

New medoids: \( (1, 1) \) and \( (9, 9) \).

Recalculate distances and assign clusters:

| Point       | Distance to \( (1, 1) \) | Distance to \( (9, 9) \) | Cluster |
|-------------|--------------------------|--------------------------|---------|
| \( (1, 1) \) | 0.0                      | \( \sqrt{(9-1)^2 + (9-1)^2} = 11.31 \) | 1       |
| \( (2, 2) \) | \( \sqrt{(2-1)^2 + (2-1)^2} = 1.41 \) | \( \sqrt{(9-2)^2 + (9-2)^2} = 9.90 \) | 1       |
| \( (3, 3) \) | \( \sqrt{(3-1)^2 + (3-1)^2} = 2.83 \) | \( \sqrt{(9-3)^2 + (9-3)^2} = 8.49 \) | 1       |
| \( (8, 8) \) | \( \sqrt{(8-1)^2 + (8-1)^2} = 9.90 \) | \( \sqrt{(9-8)^2 + (9-8)^2} = 1.41 \) | 2       |
| \( (9, 9) \) | \( \sqrt{(9-1)^2 + (9-1)^2} = 11.31 \) | 0.0                              | 2       |
| \( (10, 10) \) | \( \sqrt{(10-1)^2 + (10-1)^2} = 12.73 \) | \( \sqrt{(10-9)^2 + (10-9)^2} = 1.41 \) | 2       |

**New Clusters:**
- Cluster 1: \( (1, 1), (2, 2), (3, 3) \)
- Cluster 2: \( (8, 8), (9, 9), (10, 10) \)

#### New Total Dissimilarity:
\[
\text{Cluster 1: } 0 + 1.41 + 2.83 = 4.24
\]
\[
\text{Cluster 2: } 1.41 + 0 + 1.41 = 2.82
\]

**New Total = \( 4.24 + 2.82 = 7.06 \)**

#### Result:
The new total dissimilarity \( 7.06 \) is lower than the initial \( 8.48 \), so swapping \( (8, 8) \) with \( (9, 9) \) improves clustering.

---

## Conclusion

K-Medoid clustering is a powerful tool for robust and interpretable clustering when working with noisy datasets or datasets requiring non-Euclidean distance metrics. Although computationally more expensive than K-Means, its robustness makes it a valuable choice for various applications.

Whether you're segmenting customers or detecting anomalies, K-Medoid offers a reliable alternative to traditional clustering techniques.