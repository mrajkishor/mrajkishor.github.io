
# 📉 Dimensionality Reduction Techniques & Related Concepts

---

## 📌 **1. Dimensionality Reduction Techniques**

Dimensionality reduction is the process of reducing the number of input variables (features) in a dataset, while **retaining as much significant information as possible**. This helps:

* Avoid the *curse of dimensionality*
* Prevent overfitting
* Reduce computation time
* Improve data visualization

---

### 🔹 **1.1 Principal Component Analysis (PCA)**

PCA is a **linear** dimensionality reduction technique that transforms original features into **uncorrelated principal components**, ordered by the amount of variance they explain.

#### 📌 Steps:

1. **Standardization** of data (mean = 0, variance = 1)
2. **Covariance matrix** computation
3. Compute **eigenvectors and eigenvalues**
4. Select top-k eigenvectors with highest eigenvalues (principal components)
5. **Project original data** onto the new feature space

#### ✅ Applications:

* Finance (factor analysis)
* Image compression
* Bioinformatics

#### 🖼️ Visual Aid:

Imagine a cloud of points in 3D space projected to 2D such that it still preserves maximum variance.

---

### 🔹 **1.2 Eigenfaces**

An application of PCA for **face recognition**.

#### 📌 Steps:

1. **Flatten images** into vectors (e.g., 100×100 → 10,000D vector)
2. Subtract the **mean face**
3. Compute **covariance matrix** of centered vectors
4. Extract **eigenvectors** (eigenfaces)
5. Each face is now represented as a **linear combination** of eigenfaces

#### ✅ Use Case:

* Facial recognition systems (e.g., security, biometric authentication)

---

### 🔹 **1.3 Multidimensional Scaling (MDS)**

A **linear** technique for reducing dimensions by preserving **pairwise distances**.

#### 📌 Steps:

1. Compute **distance/dissimilarity matrix**
2. Choose target dimension (usually 2 or 3)
3. **Optimize** positions of points in low-dimensional space to minimize "stress" (difference in pairwise distances)

#### ✅ Applications:

* Psychology (perceptual mapping)
* Marketing (brand perception studies)

---

### 🔹 **1.4 Non-Linear Methods**

These are useful when data lies on a **non-linear manifold** in high-dimensional space.

#### a. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**

* Preserves **local structure**
* Maps high-dimensional data to 2D/3D by minimizing KL-divergence between high-dimensional and low-dimensional distributions
* Ideal for **visualization of clusters**

#### b. **Isomap**

* Captures **geodesic distances** using shortest paths between data points
* Useful for curved manifolds like **image or voice data**

#### ✅ Real-World Use:

* Genomic and gene expression visualization
* NLP embeddings
* Image clustering

---

## 📌 **2. Graph-Based Semi-Supervised Learning and Representation Learning**

---

### 🔹 **2.1 Semi-Supervised Learning (SSL)**

Involves training models using **few labeled** and **many unlabeled** data points.

#### 📌 Key Steps:

1. **Graph Construction**: Nodes = data points, edges = similarity
2. **Label Propagation**: Use algorithms like *Label Spreading* to diffuse known labels across graph
3. **Model Training**: Retrain on full (expanded) labeled dataset

#### ✅ Applications:

* Document classification
* Fraud detection
* Speech and image tagging

---

### 🔹 **2.2 Representation Learning with Graphs**

Learns **vector representations (embeddings)** of nodes or graphs for use in classification, clustering, or recommendation.

#### 📌 Techniques:

**a. Node2Vec / DeepWalk:**

* Perform **random walks** to capture context of nodes
* Use word2vec-style training to learn node embeddings

**b. Graph Neural Networks (GNNs):**

* Each node **aggregates features** from neighbors
* Iteratively updates representations
* Can learn node-, edge-, or graph-level embeddings

#### ✅ Applications:

* Friend recommendations
* Drug discovery (molecular graph learning)
* Knowledge graph completion

---

## 📌 **3. Feature Subset Selection & Similarity Metrics**

---

### 🔹 **3.1 Feature Subset Selection**

Goal: Select the most relevant subset of features to improve model performance and reduce complexity.

#### 📌 Methods:

**a. Filter Methods**

* Independent of any ML algorithm
* Use:

  * Pearson correlation
  * Chi-square test
  * Mutual Information

**b. Wrapper Methods**

* Use model accuracy to evaluate subsets
* Includes:

  * Forward Selection
  * Backward Elimination
  * Recursive Feature Elimination (RFE)

**c. Embedded Methods**

* Feature selection occurs during model training
* Examples:

  * Lasso (L1 regularization)
  * Decision Trees (select important features via splits)

---

### 🔹 **3.2 Distance and Similarity Metrics**

Used in clustering, KNN, and recommender systems.

#### 📌 Common Measures:

| Metric                 | Use Case                     | Formula / Behavior                            |             |    |          |    |
| ---------------------- | ---------------------------- | --------------------------------------------- | ----------- | -- | -------- | -- |
| **Euclidean**          | Straight-line distance (KNN) | $\sqrt{\sum (x_i - y_i)^2}$                   |             |    |          |    |
| **Manhattan**          | Grid/city block distances    | (\sum                                         | x\_i - y\_i | )  |          |    |
| **Cosine Similarity**  | Text, angle-based similarity | $\cos(\theta) = \frac{A \cdot B}{\|A\|\|B\|}$ |             |    |          |    |
| **Jaccard Similarity** | Set similarity (binary data) | (\frac{                                       | A \cap B    | }{ | A \cup B | }) |

#### ✅ Applications:

* Cosine: Document similarity
* Euclidean: K-means clustering
* Jaccard: Customer purchase patterns

---

## ✅ Summary Table

| **Technique**                | **Purpose**                       | **Ideal Use Case**                |
| ---------------------------- | --------------------------------- | --------------------------------- |
| PCA                          | Linear dimensionality reduction   | Finance, general ML pipelines     |
| Eigenfaces                   | Face recognition using PCA        | Biometric authentication          |
| MDS                          | Preserves pairwise distances      | Brand perception, psychology      |
| t-SNE                        | Visualize local structure         | Genomics, NLP, clustering         |
| Isomap                       | Preserves geodesic distances      | Curved manifolds like image/audio |
| Semi-Supervised Learning     | Leverage labeled + unlabeled data | Document or image classification  |
| GNNs, Node2Vec               | Graph-based embeddings            | Social networks, drug prediction  |
| Feature Subset Selection     | Improve accuracy & efficiency     | Any ML model with many features   |
| Euclidean/Manhattan Distance | Similarity measure                | Clustering, KNN                   |
| Cosine/Jaccard Similarity    | Angular or set-based similarity   | NLP, recommendation engines       |

