

# **Chapter 4 – Threads (Sections 4.1–4.3)**

---

## **4.1 Overview**

### **What is a Thread?**

* A **thread** is the **smallest unit of CPU execution** within a process.
* Shares the process’s:

  * Code section
  * Data section
  * Files/resources
* Has its own:

  * Program counter (PC)
  * Stack
  * Registers

**Analogy:**
A process is like a program in memory, and threads are workers within it that can run different tasks **concurrently**.

---

### **Why Use Threads?**

* **Responsiveness:** One thread can handle user interaction while others continue background work.
* **Resource Sharing:** Threads in same process share memory/resources → no explicit IPC needed.
* **Economy:** Thread creation/switching is faster than process creation/context switching.
* **Scalability:** Multi-core systems can run threads truly in parallel.

---

### **Multithreaded Programming Examples**

* **Web browsers:** One thread for rendering, another for fetching data.
* **Word processors:** Spell-check thread + autosave thread + user typing thread.
* **Servers:** Each client connection handled by a separate thread.

---

## **4.2 Multicore Programming**

Modern CPUs have **multiple cores** — making multithreading more critical.

### **Concurrency vs Parallelism**

* **Concurrency:** Multiple threads *in progress* at the same time (may be interleaved on single core).
* **Parallelism:** Threads literally running *at the same time* on multiple cores.

---

### **Challenges in Multicore Programming**

1. **Dividing activities** – Break task into independent parts.
2. **Balance** – Equal load across cores.
3. **Data splitting** – Avoid excessive sharing to reduce contention.
4. **Data dependency** – Handle cases where one thread depends on another’s output.
5. **Testing/debugging** – Harder due to race conditions and non-deterministic execution.

---

### **Types of Parallelism**

* **Data Parallelism:** Same task on different chunks of data.
* **Task Parallelism:** Different tasks run in parallel.

---

### **Amdahl’s Law** *(GATE favorite formula)*

If **S** = fraction of program that is *serial* and **N** = number of processors:

$$
\text{Speedup} \le \frac{1}{S + \frac{1-S}{N}}
$$

* Shows **diminishing returns**: adding cores helps only if parallel portion is large.

---

## **4.3 Multithreading Models**

Processes can be designed with **user threads** and **kernel threads**.

### **User Threads**

* Managed entirely in user space by a **thread library**.
* Kernel knows nothing about them.
* **Pros:** Very fast to create/switch.
* **Cons:** If one thread makes a blocking system call → all threads in process block.

### **Kernel Threads**

* Managed by OS kernel.
* **Pros:** True concurrency — OS can schedule threads individually.
* **Cons:** Higher overhead for creation/switch.

---

### **User–Kernel Mapping Models**

#### **1. Many-to-One**

* Many user threads → single kernel thread.
* **Pros:** Simple, efficient.
* **Cons:** No parallelism on multi-core; blocking call stops all threads.
* **Example:** Early Java Green Threads.

#### **2. One-to-One**

* Each user thread → one kernel thread.
* **Pros:** Parallelism possible; blocking call doesn’t affect others.
* **Cons:** High overhead; kernel limits thread count.
* **Example:** Windows, Linux (POSIX Pthreads).

#### **3. Many-to-Many**

* Many user threads → smaller/equal number of kernel threads.
* **Pros:** Combines flexibility + concurrency.
* **Cons:** More complex to implement.
* **Example:** Windows ThreadFiber model, some UNIX variants.

---

## **GATE Exam Pointers**

* Know **difference** between process & thread (memory sharing is the key).
* **Amdahl’s Law** formula & interpretation frequently appear in numericals.
* Thread models mapping (Many-to-One, One-to-One, Many-to-Many) often in match-the-following format.
* Concurrency vs Parallelism definitions are a common 1-mark question.


