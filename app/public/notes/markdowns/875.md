

## üßÆ Floating Point Arithmetic

### üìå Definition

**Floating-point numbers** are numbers where the position of the *radix point* (also known as the decimal point or binary point) is not fixed but can *float*. Unlike fixed-point numbers‚Äîwhere the decimal position is pre-defined‚Äîfloating-point allows a much **wider range of values**, including very small and very large numbers, to be represented.

---

### ‚öôÔ∏è Why Floating Point?

* **Fixed-point limitations**: Cannot accurately represent all fractions or extremely large/small numbers.
* **Floating-point advantage**: Supports a **much broader dynamic range** by adjusting the position of the radix point based on an exponent.

---

### üî£ Floating Point Representation (IEEE 754 Standard)

A floating-point number is typically represented in three parts:

* **Sign bit (s)**: 0 for positive, 1 for negative
* **Exponent (E)**: Encodes the position of the decimal point
* **Mantissa or Significand (M)**: Stores the precision bits of the number

### Formula:

$$
F = (-1)^s \times M \times 2^E
$$

IEEE formats (examples):

* **Single Precision (32-bit)**: 1 bit sign, 8 bits exponent, 23 bits mantissa
* **Double Precision (64-bit)**: 1 bit sign, 11 bits exponent, 52 bits mantissa

Bias values (to allow for negative exponents):

* **Single precision bias**: 127
* **Double precision bias**: 1023

---

### üîß Floating Point Operations

Most programming languages support a `float`, `double`, or `real` data type that uses **floating-point arithmetic**. These operations rely on **dedicated FP units (hardware)** inside the CPU or GPU, and are significantly more **complex and slower** than integer arithmetic.

#### ‚úÖ Supported operations:

* Addition
* Subtraction
* Multiplication
* Division

---

### ‚ûï FP Addition/Subtraction Algorithm

Let `X = Mx √ó 2^Ex` and `Y = My √ó 2^Ey` be the two operands. Assume `Ey > Ex`.

**Steps:**

1. **Align exponents**:
   Shift the mantissa of the smaller exponent right:
   `Mx ‚Üí Mx √ó 2^(Ey - Ex)`
   (so both operands now have exponent `Ey`)
2. **Add/Subtract mantissas**:
   Add `Mx` and `My` (based on operation)
3. **Determine sign** of result
4. **Normalize** the result:
   Shift mantissa so it fits the expected range (e.g., \[1, 2) in binary)
5. **Round** (if necessary)
6. **Reassemble** the floating-point number from sign, exponent, and mantissa

---

### ‚úñÔ∏è FP Multiplication Algorithm

Steps:

1. **Add exponents**:
   `Eresult = Ex + Ey - Bias`
2. **Multiply mantissas**:
   `Mresult = Mx √ó My`
3. **Normalize** and round the result
4. **Determine sign**:
   `Sresult = Sx XOR Sy`

---

### ‚ûó FP Division Algorithm

Steps:

1. **Subtract exponents**:
   `Eresult = Ex - Ey + Bias`
2. **Divide mantissas**:
   `Mresult = Mx √∑ My`
3. **Normalize** and round
4. **Determine sign**

---

### üìâ Normalization

Normalization ensures the mantissa is within a defined range, usually:

* **Binary**: Mantissa is normalized to be of the form `1.xxxxxx`
* This avoids multiple representations of the same number and ensures maximum precision

---

### ‚ö†Ô∏è Floating Point Challenges

* **Precision Loss**: Due to limited bits in mantissa, some values can't be represented exactly
* **Rounding Errors**: Small errors accumulate over multiple operations
* **Complex Hardware**: Requires more circuitry and power
* **Not Associative**: Unlike integers, FP addition/subtraction is not always associative
  i.e., `(a + b) + c ‚â† a + (b + c)` in FP

---

### ‚úÖ Summary of Advantages

* Wide dynamic range
* Can represent both very large and very small numbers
* Backed by IEEE 754 standard
* Available in hardware (FPU)

