# **Hexagonal Architecture (Ports & Adapters) in Microservices: A Deep Dive**  

## **🚀 Introduction**  
In modern software development, **maintainability, testability, and flexibility** are crucial. **Hexagonal Architecture (Ports & Adapters)** provides a way to **decouple business logic from external dependencies**, making applications **more modular and resilient to change**.

This blog covers:  
- **What is Hexagonal Architecture?**  
- **Ports & Adapters Explained**  
- **How It Differs from Layered Architecture**  
- **Benefits of Hexagonal Architecture**  
- **Hands-on Example in Spring Boot**  

---

## **📌 What is Hexagonal Architecture?**
Hexagonal Architecture, also known as **Ports & Adapters**, was introduced by **Alistair Cockburn** to **separate core business logic from external systems** (e.g., databases, APIs, UIs).

### **🔹 Core Principles**
✅ **Business Logic Should Be Independent** – It should not depend on frameworks, databases, or UI components.  
✅ **External Systems Should Plug into the Application** – Instead of the application depending on them.  
✅ **Easier Testability & Maintainability** – Since dependencies are externalized, unit testing is simplified.  

### **🔹 Structure of Hexagonal Architecture**
1. **Core Domain (Inside the Hexagon)** – Contains **pure business logic**.  
2. **Ports (Input & Output Interfaces)** – Define how the application interacts with the outside world.  
3. **Adapters (Implementation of Ports)** – Implement ports for **database, REST APIs, messaging systems, etc.**  

### **🔹 High-Level Diagram**
```
                      ┌──────────────────────┐
      UI / API  --->  │    Application Core  │  <--- Database / API Calls
                      └──────────────────────┘
```
- **Core Application** contains **business logic**.  
- **Ports** define interaction contracts.  
- **Adapters** connect external systems (DB, APIs, messaging).  

---

## **📌 Ports & Adapters Explained**
### **🔹 Ports (Interfaces)**
Ports are **abstractions** (interfaces) that define how external components interact with the application.

**Types of Ports:**
- **Inbound Ports (Driving Adapters)** → How external inputs reach the application.
  - Examples: REST controllers, GraphQL APIs, CLI interfaces.
- **Outbound Ports (Driven Adapters)** → How the application communicates with external services.
  - Examples: Database repositories, third-party APIs, messaging queues.

### **🔹 Adapters (Implementations)**
Adapters **implement Ports** to interact with external systems.

**Types of Adapters:**
- **Inbound Adapters** → Controllers, message consumers.
- **Outbound Adapters** → Database repositories, API clients.

---

## **📌 How Hexagonal Architecture Differs from Layered Architecture**
| **Aspect**        | **Layered Architecture**              | **Hexagonal Architecture**            |
|------------------|------------------------------------|------------------------------------|
| **Structure**    | Strictly divided into layers (Controller → Service → Repository) | Components interact via ports & adapters |
| **Dependency**   | Business logic **depends on** frameworks (Spring, Hibernate) | Business logic **independent** of frameworks |
| **Testability**  | Harder due to direct framework dependencies | Easier due to **mockable interfaces** |
| **Flexibility**  | Hard to swap databases, APIs | Easy to replace adapters |

✅ **Hexagonal Architecture provides better modularity and testability** compared to traditional **Layered Architecture**.

---

## **📌 Hands-on Implementation in Spring Boot**
### **💡 Use Case: Order Management System**
- **Inbound Port:** `OrderService` (Business Logic)
- **Inbound Adapter:** `OrderController` (REST API)
- **Outbound Port:** `OrderRepositoryPort` (Database Interface)
- **Outbound Adapter:** `JpaOrderRepositoryAdapter` (Implements DB logic)

---

### **🔹 Step 1: Define the Core Business Logic (Inside the Hexagon)**
#### **📌 Define `Order` Entity**
```java
@Entity
public class Order {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private String product;
    private int quantity;
}
```

#### **📌 Define `OrderService` (Inbound Port)**
```java
public interface OrderService {
    Order createOrder(Order order);
    Order getOrderById(Long id);
}
```

#### **📌 Implement Business Logic**
```java
@Service
public class OrderServiceImpl implements OrderService {
    private final OrderRepositoryPort orderRepository;

    @Autowired
    public OrderServiceImpl(OrderRepositoryPort orderRepository) {
        this.orderRepository = orderRepository;
    }

    @Override
    public Order createOrder(Order order) {
        return orderRepository.save(order);
    }

    @Override
    public Order getOrderById(Long id) {
        return orderRepository.findById(id);
    }
}
```
✅ **The business logic doesn’t depend on any database or framework!** 🎉

---

### **🔹 Step 2: Implement Inbound Adapter (REST API Controller)**
```java
@RestController
@RequestMapping("/orders")
public class OrderController {
    private final OrderService orderService;

    @Autowired
    public OrderController(OrderService orderService) {
        this.orderService = orderService;
    }

    @PostMapping
    public ResponseEntity<Order> createOrder(@RequestBody Order order) {
        return ResponseEntity.ok(orderService.createOrder(order));
    }

    @GetMapping("/{id}")
    public ResponseEntity<Order> getOrder(@PathVariable Long id) {
        return ResponseEntity.ok(orderService.getOrderById(id));
    }
}
```
✅ **The controller doesn’t directly depend on the database!**  

---

### **🔹 Step 3: Define Outbound Port (Database Interface)**
```java
public interface OrderRepositoryPort {
    Order save(Order order);
    Order findById(Long id);
}
```
✅ **This interface allows us to swap databases without changing the business logic.**  

---

### **🔹 Step 4: Implement Outbound Adapter (Database Repository)**
```java
@Repository
public class JpaOrderRepositoryAdapter implements OrderRepositoryPort {
    private final JpaOrderRepository jpaOrderRepository;

    @Autowired
    public JpaOrderRepositoryAdapter(JpaOrderRepository jpaOrderRepository) {
        this.jpaOrderRepository = jpaOrderRepository;
    }

    @Override
    public Order save(Order order) {
        return jpaOrderRepository.save(order);
    }

    @Override
    public Order findById(Long id) {
        return jpaOrderRepository.findById(id).orElse(null);
    }
}
```

✅ **This adapter acts as a bridge between the database and the application logic.**  

---

### **🔹 Step 5: Define JPA Repository (Database Interface)**
```java
public interface JpaOrderRepository extends JpaRepository<Order, Long> {
}
```

✅ **Now, the database implementation can be changed without modifying the business logic!** 🎉  

---

## **📌 Benefits of Hexagonal Architecture**
✅ **Decouples business logic from external dependencies**  
✅ **Easier unit testing (Mock dependencies instead of using actual DB)**  
✅ **Supports multiple input methods (REST API, CLI, Messaging)**  
✅ **Allows database changes without impacting the business logic**  
✅ **Highly scalable and maintainable**  

---

## **📌 Conclusion**
### **🚀 What We Achieved**
✅ **Designed a Hexagonal Architecture using Ports & Adapters.**  
✅ **Decoupled business logic from frameworks & databases.**  
✅ **Made the system easily extensible & testable.**  
✅ **Built an Order Management Microservice using Spring Boot.**  

🔹 **Next Steps:**  
- Implement **event-driven messaging (Kafka, RabbitMQ) as an outbound adapter**.  
- Add **Circuit Breakers (Resilience4J) for fault tolerance**.  
- Integrate with **GraphQL, gRPC as additional inbound adapters**.  

---

# **Integrating Event-Driven Messaging in Hexagonal Architecture with Kafka (Ports & Adapters)**  

## **🚀 Introduction**  
In microservices, using **event-driven communication** improves **scalability, decoupling, and reliability**. **Hexagonal Architecture (Ports & Adapters)** helps achieve this by **defining clear input (ports) and output (adapters) interfaces** for messaging.

📌 **In this guide, we will:**  
✅ **Add Kafka as an outbound adapter** to publish events from the `OrderService`.  
✅ **Add Kafka as an inbound adapter** to consume events in `InventoryService`.  
✅ **Ensure the architecture remains clean and modular** with Hexagonal principles.  

---

## **📌 Step 1: Add Dependencies for Kafka**
In both `order-service` and `inventory-service`, add **Spring Boot Kafka dependencies** in `pom.xml`:

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

✅ **This enables both Kafka producer (publishing events) and Kafka consumer (listening to events).**  

---

## **📌 Step 2: Define the Outbound Port for Messaging (Order Event Publisher)**
Instead of directly using Kafka in our service, we **define an interface (port) to send messages**.

### **🔹 Create `OrderEventPublisherPort` Interface**
📌 This is the **outbound port** that allows `OrderService` to send order events.
```java
public interface OrderEventPublisherPort {
    void publishOrderCreatedEvent(Order order);
}
```

✅ **This keeps our business logic decoupled from Kafka!** 🎉  

---

## **📌 Step 3: Implement the Outbound Adapter (Kafka Publisher)**
📌 This adapter **publishes messages** to the Kafka `order-events` topic.

```java
@Service
public class KafkaOrderEventPublisherAdapter implements OrderEventPublisherPort {
    private final KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate;

    @Autowired
    public KafkaOrderEventPublisherAdapter(KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    @Override
    public void publishOrderCreatedEvent(Order order) {
        OrderCreatedEvent event = new OrderCreatedEvent(order.getId(), order.getProduct(), order.getQuantity());
        kafkaTemplate.send("order-events", event);
    }
}
```

✅ **Now, `OrderService` can publish order events without being coupled to Kafka.**  

---

## **📌 Step 4: Modify Order Service to Publish Events**
📌 `OrderService` **calls the outbound port** to notify other services.

```java
@Service
public class OrderServiceImpl implements OrderService {
    private final OrderRepositoryPort orderRepository;
    private final OrderEventPublisherPort eventPublisher;

    @Autowired
    public OrderServiceImpl(OrderRepositoryPort orderRepository, OrderEventPublisherPort eventPublisher) {
        this.orderRepository = orderRepository;
        this.eventPublisher = eventPublisher;
    }

    @Override
    public Order createOrder(Order order) {
        Order savedOrder = orderRepository.save(order);
        eventPublisher.publishOrderCreatedEvent(savedOrder); // Publish event
        return savedOrder;
    }
}
```

✅ **Now, whenever an order is created, an event is published to Kafka!** 🎉  

---

## **📌 Step 5: Define the Inbound Port for Order Events in Inventory Service**
📌 Instead of directly using Kafka, **we define a port (interface) to consume events**.

```java
public interface OrderEventConsumerPort {
    void processOrderCreatedEvent(OrderCreatedEvent event);
}
```

✅ **This keeps `InventoryService` decoupled from Kafka!**  

---

## **📌 Step 6: Implement the Inbound Adapter (Kafka Consumer)**
📌 This adapter **listens to Kafka messages** and calls the **inbound port**.

```java
@Service
public class KafkaOrderEventConsumerAdapter {
    private final OrderEventConsumerPort orderEventConsumerPort;

    @Autowired
    public KafkaOrderEventConsumerAdapter(OrderEventConsumerPort orderEventConsumerPort) {
        this.orderEventConsumerPort = orderEventConsumerPort;
    }

    @KafkaListener(topics = "order-events", groupId = "inventory-group")
    public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
        orderEventConsumerPort.processOrderCreatedEvent(event);
    }
}
```

✅ **Kafka events are now passed to the inbound port (`OrderEventConsumerPort`) instead of being handled directly.**  

---

## **📌 Step 7: Implement Business Logic in Inventory Service**
📌 `InventoryService` listens for `OrderCreatedEvent` and updates stock.

```java
@Service
public class InventoryServiceImpl implements OrderEventConsumerPort {
    private final InventoryRepository inventoryRepository;

    @Autowired
    public InventoryServiceImpl(InventoryRepository inventoryRepository) {
        this.inventoryRepository = inventoryRepository;
    }

    @Override
    public void processOrderCreatedEvent(OrderCreatedEvent event) {
        System.out.println("Updating inventory for order: " + event.getOrderId());
        inventoryRepository.updateStock(event.getProduct(), event.getQuantity());
    }
}
```

✅ **Now, inventory updates happen whenever an order is created!** 🎉  

---

## **📌 Step 8: Set Up Kafka in Docker Compose**
Add the **Kafka and Zookeeper** services in `docker-compose.yml`:

```yaml
version: '3'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"

  kafka:
    image: wurstmeister/kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
    depends_on:
      - zookeeper
```

✅ **Kafka is now running on `localhost:9092`.**  

---

## **📌 Step 9: Configure Kafka in `application.yml`**
For **Order Service**:
```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
```

For **Inventory Service**:
```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: inventory-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: "*"
```

✅ **Kafka is now configured for both producer & consumer!**  

---

## **📌 Step 10: Testing the Integration**
### **🔹 Start the Microservices & Kafka**
Run the following command:
```bash
docker-compose up -d
```

### **🔹 Create an Order via API**
```bash
curl -X POST http://localhost:8081/orders \
     -H "Content-Type: application/json" \
     -d '{ "product": "Laptop", "quantity": 2 }'
```
✅ **This will trigger an event and update inventory.**  

### **🔹 Check Kafka Logs**
Run:
```bash
docker logs -f inventory-service
```
✅ **You should see:**  
```
Updating inventory for order: 1
```
✅ **The event-driven architecture is working perfectly!** 🎉  

---

## **📌 Final Architecture**
```
                      ┌──────────────────────┐
 Order API  --->      │    Order Service     │
                      │     (Business)       │
                      └────────▲─────────────┘
                               │
                    ┌──────────┴──────────┐
                    │ Kafka (order-events) │
                    └──────────▲──────────┘
                               │
                      ┌────────▼──────────┐
                      │  Inventory Service │
                      │      (Stock)       │
                      └───────────────────┘
```

✅ **Orders are created asynchronously without direct service calls!**  

---

## **📌 Conclusion**
### **🚀 What We Achieved**
✅ **Implemented Kafka as an event-driven adapter in Hexagonal Architecture.**  
✅ **Used Ports & Adapters to keep business logic clean and decoupled.**  
✅ **Made Order & Inventory Services communicate asynchronously.**  
✅ **Tested the system using Docker and APIs.**  

🔹 **Next Steps:**  
- Implement **Resilience Patterns (Circuit Breaker, Retry, Dead Letter Queue)**.  
- Optimize **Kafka event retention & partitioning** for scalability.  

---

# **Implementing Resilience Patterns in Kafka-Based Microservices**  

## **🚀 Introduction**  
In microservices, **failures are inevitable**. A **Kafka-based event-driven system** needs **resilience patterns** to handle:  
- **Network failures**  
- **Message loss**  
- **Duplicate messages**  
- **Consumer crashes**  

📌 **In this guide, we will:**  
✅ **Add Circuit Breakers & Retries using Resilience4j.**  
✅ **Handle failed messages using Dead Letter Queue (DLQ).**  
✅ **Implement Idempotency to prevent duplicate processing.**  
✅ **Use Error Handling in Kafka Consumers.**  

---

## **📌 Step 1: Implement Circuit Breaker with Resilience4j**
A **Circuit Breaker** prevents the system from sending too many failing requests. If a service keeps failing, it **opens** the circuit and stops sending requests temporarily.

### **🔹 Add Dependencies**
In `pom.xml`, add:

```xml
<dependency>
    <groupId>io.github.resilience4j</groupId>
    <artifactId>resilience4j-spring-boot2</artifactId>
</dependency>
```

---

### **🔹 Configure Circuit Breaker in `application.yml`**
```yaml
resilience4j:
  circuitbreaker:
    instances:
      kafkaListener:
        slidingWindowSize: 10
        failureRateThreshold: 50
        waitDurationInOpenState: 5000ms
```
- **Opens if failure rate > 50% in last 10 calls.**
- **Waits for 5 seconds before retrying.**

---

### **🔹 Apply Circuit Breaker to Kafka Consumer**
📌 **Wrap the Kafka consumer logic inside a circuit breaker.**

```java
@Service
public class KafkaOrderConsumer {
    private final InventoryService inventoryService;

    @Autowired
    public KafkaOrderConsumer(InventoryService inventoryService) {
        this.inventoryService = inventoryService;
    }

    @KafkaListener(topics = "order-events", groupId = "inventory-group")
    @CircuitBreaker(name = "kafkaListener", fallbackMethod = "handleFailure")
    public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
        inventoryService.updateStock(event.getProduct(), event.getQuantity());
    }

    public void handleFailure(OrderCreatedEvent event, Exception ex) {
        System.out.println("Circuit Breaker OPEN! Order processing failed: " + event.getOrderId());
    }
}
```
✅ **Now, if the consumer keeps failing, it stops processing for 5 seconds before retrying.** 🎉  

---

## **📌 Step 2: Implement Retry Mechanism**
If a service temporarily fails, a **Retry Mechanism** can help recover without immediately failing.

### **🔹 Configure Retry in `application.yml`**
```yaml
resilience4j:
  retry:
    instances:
      kafkaRetry:
        maxAttempts: 3
        waitDuration: 2s
```
- **Retries a failed message 3 times with a 2-second delay.**

---

### **🔹 Apply Retry to Kafka Consumer**
📌 **If processing fails, the retry mechanism will retry up to 3 times before marking it as a failure.**

```java
@Retry(name = "kafkaRetry")
@KafkaListener(topics = "order-events", groupId = "inventory-group")
public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
    inventoryService.updateStock(event.getProduct(), event.getQuantity());
}
```

✅ **Now, the consumer retries failed messages before giving up.**  

---

## **📌 Step 3: Implement Dead Letter Queue (DLQ) for Failed Messages**
A **Dead Letter Queue (DLQ)** stores messages that **failed after multiple retries**, preventing message loss.

### **🔹 Configure Kafka with DLQ**
📌 **Modify `application.yml` to send failed messages to `order-events-dlq`.**

```yaml
spring:
  kafka:
    consumer:
      enable-auto-commit: false
      properties:
        spring.kafka.listener.ack-mode: manual
      topic:
        order-events: order-events
        dlq: order-events-dlq
```

---

### **🔹 Modify Consumer to Send Failed Messages to DLQ**
📌 **If a message fails even after retries, send it to the DLQ.**

```java
@KafkaListener(topics = "order-events", groupId = "inventory-group")
public void consumeOrderCreatedEvent(OrderCreatedEvent event, Acknowledgment acknowledgment) {
    try {
        inventoryService.updateStock(event.getProduct(), event.getQuantity());
        acknowledgment.acknowledge(); // Acknowledge only on success
    } catch (Exception e) {
        kafkaTemplate.send("order-events-dlq", event); // Send to Dead Letter Queue
    }
}
```

✅ **Now, failed messages go to `order-events-dlq` instead of being lost!** 🎉  

---

## **📌 Step 4: Implement Idempotency to Prevent Duplicate Processing**
If the same event is processed multiple times (e.g., after a consumer restart), **idempotency ensures we don't process the same order twice.**

### **🔹 Add an Idempotency Check in Inventory Service**
📌 **Store processed order IDs to avoid duplicate processing.**

```java
@Service
public class InventoryService {
    private final Set<Long> processedOrders = Collections.synchronizedSet(new HashSet<>());

    public void updateStock(String product, int quantity, Long orderId) {
        if (processedOrders.contains(orderId)) {
            System.out.println("Skipping duplicate order: " + orderId);
            return;
        }
        processedOrders.add(orderId);
        System.out.println("Processing order: " + orderId);
    }
}
```
✅ **Even if the same message is received twice, it will be ignored.**  

---

## **📌 Step 5: Handle Errors in Kafka Consumers**
📌 **Instead of crashing, the consumer logs errors and moves on.**

```java
@KafkaListener(topics = "order-events", groupId = "inventory-group", errorHandler = "kafkaErrorHandler")
public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
    inventoryService.updateStock(event.getProduct(), event.getQuantity());
}

@Bean
public KafkaListenerErrorHandler kafkaErrorHandler() {
    return (message, exception) -> {
        System.out.println("Error processing Kafka message: " + message.getPayload());
        return null; // Move to next message
    };
}
```

✅ **Now, Kafka consumers handle failures without stopping!** 🎉  

---

## **📌 Final Architecture**
```
                      ┌──────────────────────┐
 Order API  --->      │    Order Service     │
                      │     (Business)       │
                      └────────▲─────────────┘
                               │
                    ┌──────────┴──────────┐
                    │ Kafka (order-events) │
                    └──────────▲──────────┘
                               │
      ┌────────────────────────┴──────────────────────────┐
      │            Resilience Patterns Applied             │
      ├───────────────────────────────────────────────────┤
      │ ✅ Circuit Breaker (Stops failing consumers)      │
      │ ✅ Retries (Retries 3 times before failing)       │
      │ ✅ Dead Letter Queue (Stores unprocessed events)  │
      │ ✅ Idempotency (Prevents duplicate processing)    │
      │ ✅ Error Handling (Logs errors & continues)      │
      └───────────────────────────────────────────────────┘
                               │
                      ┌────────▼──────────┐
                      │  Inventory Service │
                      │      (Stock)       │
                      └───────────────────┘
```

✅ **Microservices are now more fault-tolerant and resilient!** 🎉  

---

## **📌 Conclusion**
### **🚀 What We Achieved**
✅ **Implemented Circuit Breaker to prevent cascading failures.**  
✅ **Added Retry Mechanism to recover from temporary failures.**  
✅ **Set up Dead Letter Queue (DLQ) for failed messages.**  
✅ **Ensured Idempotency to prevent duplicate processing.**  
✅ **Handled Kafka Consumer Errors gracefully.**  

🔹 **Next Steps:**  
- Implement **Distributed Tracing (Zipkin) to track Kafka messages across services**.  
- Use **Kafka Schema Registry (Avro) to handle evolving event structures**.  

---

# **Implementing Kafka Schema Registry & Distributed Tracing in Microservices**  

## **🚀 Introduction**  
When working with **Kafka in microservices**, managing **schema evolution** and **tracing events across services** is crucial.  

📌 **In this guide, we will:**  
✅ **Implement Kafka Schema Registry (Avro) for structured messaging.**  
✅ **Enable Distributed Tracing using OpenTelemetry & Zipkin.**  
✅ **Ensure compatibility when evolving message formats.**  
✅ **Visualize event flow across microservices.**  

---

# **📌 Part 1: Implementing Kafka Schema Registry (Avro)**
## **What is Kafka Schema Registry?**  
Kafka Schema Registry helps in **storing and managing Avro schemas** for Kafka messages, ensuring compatibility between producers and consumers.  

✅ **Why use Avro instead of JSON?**  
- **Compact & Efficient** → Avro **reduces message size**.  
- **Schema Evolution Support** → Producers & Consumers can use **different versions** of schemas.  
- **Type Safety** → Enforces **strict schema validation**.  

---

## **📌 Step 1: Add Dependencies for Avro & Schema Registry**
In **order-service** and **inventory-service**, add the following dependencies in `pom.xml`:  

```xml
<dependency>
    <groupId>io.confluent</groupId>
    <artifactId>kafka-avro-serializer</artifactId>
    <version>7.2.1</version>
</dependency>

<dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro</artifactId>
    <version>1.11.0</version>
</dependency>
```

✅ **This enables Avro serialization & Schema Registry support.**  

---

## **📌 Step 2: Define Avro Schema for Order Events**
Create **`order-event.avsc`** inside `src/main/resources/avro/`:

```json
{
  "type": "record",
  "name": "OrderCreatedEvent",
  "namespace": "com.example.kafka",
  "fields": [
    { "name": "orderId", "type": "long" },
    { "name": "product", "type": "string" },
    { "name": "quantity", "type": "int" }
  ]
}
```

✅ **This defines the message structure for Kafka.**  

---

## **📌 Step 3: Generate Avro Java Classes**
Run the following **Maven plugin** to auto-generate Java classes:

```xml
<plugin>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro-maven-plugin</artifactId>
    <version>1.11.0</version>
    <executions>
        <execution>
            <phase>generate-sources</phase>
            <goals>
                <goal>schema</goal>
            </goals>
            <configuration>
                <sourceDirectory>${project.basedir}/src/main/resources/avro</sourceDirectory>
                <outputDirectory>${project.build.directory}/generated-sources</outputDirectory>
            </configuration>
        </execution>
    </executions>
</plugin>
```

Run:
```bash
mvn clean install
```

✅ **Avro-generated Java classes are now available in `target/generated-sources/`!** 🎉  

---

## **📌 Step 4: Configure Kafka Producer to Use Avro**
Modify `application.yml`:

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    properties:
      schema.registry.url: http://localhost:8081
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
```

---

## **📌 Step 5: Publish Avro Messages in `OrderService`**
Modify **`KafkaOrderEventPublisherAdapter`**:

```java
@Service
public class KafkaOrderEventPublisherAdapter {
    private final KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate;

    @Autowired
    public KafkaOrderEventPublisherAdapter(KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    public void publishOrderCreatedEvent(Order order) {
        OrderCreatedEvent event = new OrderCreatedEvent(order.getId(), order.getProduct(), order.getQuantity());
        kafkaTemplate.send("order-events", event);
    }
}
```

✅ **Messages are now serialized in Avro format!**  

---

## **📌 Step 6: Configure Kafka Consumer to Use Avro**
Modify `application.yml`:

```yaml
spring:
  kafka:
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      properties:
        schema.registry.url: http://localhost:8081
        specific.avro.reader: true
```

---

## **📌 Step 7: Consume Avro Messages in `InventoryService`**
Modify **`KafkaOrderEventConsumerAdapter`**:

```java
@KafkaListener(topics = "order-events", groupId = "inventory-group")
public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
    System.out.println("Processing Avro message for Order ID: " + event.getOrderId());
    inventoryService.updateStock(event.getProduct(), event.getQuantity());
}
```

✅ **Kafka now ensures schema compatibility across services!** 🎉  

---

# **📌 Part 2: Implementing Distributed Tracing in Kafka-Based Microservices**
## **What is Distributed Tracing?**
Distributed tracing helps **track requests** as they **flow through multiple microservices**.

✅ **Why use OpenTelemetry & Zipkin?**  
- **Visualizes request flow** from producers to consumers.  
- **Helps debug latency issues.**  
- **Automatically traces Kafka messages across services.**  

---

## **📌 Step 1: Add Dependencies for OpenTelemetry & Zipkin**
In both `order-service` and `inventory-service`, add:

```xml
<dependency>
    <groupId>io.opentelemetry.instrumentation</groupId>
    <artifactId>opentelemetry-spring-boot-starter</artifactId>
    <version>1.26.0-alpha</version>
</dependency>
```

✅ **This enables auto-tracing for Spring Boot apps.**  

---

## **📌 Step 2: Configure Zipkin in `application.yml`**
Modify `application.yml`:

```yaml
otel:
  exporter:
    zipkin:
      endpoint: http://zipkin:9411/api/v2/spans
  trace:
    sampler: always_on
```

---

## **📌 Step 3: Run Zipkin in Docker Compose**
Add to `docker-compose.yml`:

```yaml
zipkin:
  image: openzipkin/zipkin
  ports:
    - "9411:9411"
```

✅ **Zipkin UI is now available at** `http://localhost:9411`  

---

## **📌 Step 4: Enable Tracing for Kafka Producer**
Modify **`KafkaOrderEventPublisherAdapter`**:

```java
@Autowired private Tracer tracer;

public void publishOrderCreatedEvent(Order order) {
    Span span = tracer.spanBuilder("publish-order-event").startSpan();
    OrderCreatedEvent event = new OrderCreatedEvent(order.getId(), order.getProduct(), order.getQuantity());
    kafkaTemplate.send("order-events", event);
    span.end();
}
```

✅ **Kafka messages now include tracing IDs!** 🎉  

---

## **📌 Step 5: Enable Tracing for Kafka Consumer**
Modify **`KafkaOrderEventConsumerAdapter`**:

```java
@KafkaListener(topics = "order-events", groupId = "inventory-group")
public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
    Span span = tracer.spanBuilder("consume-order-event").startSpan();
    inventoryService.updateStock(event.getProduct(), event.getQuantity());
    span.end();
}
```

✅ **Each service now traces events in Zipkin.**  

---

## **📌 Step 6: View Traces in Zipkin**
1️⃣ Go to `http://localhost:9411`  
2️⃣ Select **`order-service`** or **`inventory-service`**  
3️⃣ Check **request flow, latency, and dependencies**  

✅ **Kafka messages are now fully traceable across microservices!** 🎉  

---

# **📌 Conclusion**
### **🚀 What We Achieved**
✅ **Implemented Kafka Schema Registry with Avro for structured messaging.**  
✅ **Enabled Distributed Tracing using OpenTelemetry & Zipkin.**  
✅ **Ensured schema compatibility & type safety across services.**  
✅ **Visualized Kafka event flow in Zipkin.**  

🔹 **Next Steps:**  
- Implement **gRPC with Kafka for high-performance messaging**.  
- Add **security using Kafka ACLs & OAuth authentication**.  

---

# **Securing Kafka & Integrating gRPC in Microservices**

## **🚀 Introduction**  
Securing Kafka and integrating **gRPC for high-performance messaging** are critical for modern microservices.  

📌 **In this guide, we will:**  
✅ **Secure Kafka with SASL, ACLs & OAuth Authentication.**  
✅ **Implement gRPC for real-time microservice communication.**  
✅ **Use Kafka with gRPC to improve messaging performance.**  
✅ **Set up Mutual TLS for secure data exchange.**  

---

# **📌 Part 1: Securing Kafka with SASL, ACLs & OAuth**
Kafka must be secured to **prevent unauthorized access** and **ensure encrypted communication**.

## **🔹 Step 1: Enable SASL Authentication in Kafka**
SASL (Simple Authentication and Security Layer) provides **authentication** between Kafka brokers and clients.

### **📌 Modify `server.properties` in Kafka Broker**
Edit `config/server.properties`:

```properties
listeners=SASL_PLAINTEXT://:9093
advertised.listeners=SASL_PLAINTEXT://localhost:9093
sasl.mechanism.inter.broker.protocol=SCRAM-SHA-512
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer
super.users=User:kafka-admin
```

✅ **This enables SASL authentication on port `9093`**.

---

## **🔹 Step 2: Create Kafka Users with SCRAM Authentication**
```bash
kafka-configs.sh --zookeeper localhost:2181 --alter --add-config "SCRAM-SHA-512=[password=password123]" --entity-type users --entity-name order-service
kafka-configs.sh --zookeeper localhost:2181 --alter --add-config "SCRAM-SHA-512=[password=password123]" --entity-type users --entity-name inventory-service
```

✅ **Users `order-service` and `inventory-service` now have secure access to Kafka.**

---

## **🔹 Step 3: Set Up Kafka ACLs**
**Restrict topic access to specific users**:

```bash
kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 \
--add --allow-principal User:order-service --operation All --topic order-events
```

✅ **Now, only `order-service` can publish messages to `order-events`.**

---

## **🔹 Step 4: Configure OAuth Authentication**
Instead of username/password, we use **OAuth tokens for authentication**.

Modify `server.properties`:

```properties
listener.name.sasl_plaintext.oauthbearer.sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
    oauth.token.endpoint.uri="https://auth.example.com/oauth/token";
```

✅ **Kafka clients now authenticate via OAuth.**

---

## **📌 Part 2: Implementing gRPC in Microservices**
gRPC is a high-performance RPC framework **faster than REST**.

### **🔹 Step 1: Add gRPC Dependencies**
In `pom.xml`:

```xml
<dependency>
    <groupId>io.grpc</groupId>
    <artifactId>grpc-spring-boot-starter</artifactId>
    <version>2.13.1.RELEASE</version>
</dependency>
```

✅ **Now, gRPC is supported in Spring Boot.**

---

### **🔹 Step 2: Define gRPC Service (`order-service`)**
Create `order.proto` inside `src/main/proto/`:

```proto
syntax = "proto3";

service OrderService {
    rpc CreateOrder (OrderRequest) returns (OrderResponse);
}

message OrderRequest {
    string product = 1;
    int32 quantity = 2;
}

message OrderResponse {
    string status = 1;
}
```

✅ **This defines a gRPC service for orders.**  

---

### **🔹 Step 3: Generate gRPC Java Classes**
Run:

```bash
mvn clean install
```

✅ **This generates Java classes for gRPC communication.**  

---

### **🔹 Step 4: Implement gRPC Server (`order-service`)**
```java
@GRpcService
public class OrderServiceGrpcImpl extends OrderServiceGrpc.OrderServiceImplBase {
    @Override
    public void createOrder(OrderRequest request, StreamObserver<OrderResponse> responseObserver) {
        System.out.println("Received gRPC request for: " + request.getProduct());

        OrderResponse response = OrderResponse.newBuilder()
            .setStatus("Order Created")
            .build();

        responseObserver.onNext(response);
        responseObserver.onCompleted();
    }
}
```

✅ **`order-service` now serves gRPC requests.**  

---

### **🔹 Step 5: Implement gRPC Client (`inventory-service`)**
```java
@GrpcClient("order-service")
private OrderServiceGrpc.OrderServiceBlockingStub orderServiceStub;

public void placeOrder(String product, int quantity) {
    OrderRequest request = OrderRequest.newBuilder()
        .setProduct(product)
        .setQuantity(quantity)
        .build();

    OrderResponse response = orderServiceStub.createOrder(request);
    System.out.println("gRPC Response: " + response.getStatus());
}
```

✅ **Now, `inventory-service` can call `order-service` over gRPC.**  

---

## **📌 Part 3: Securing gRPC with Mutual TLS**
Mutual TLS (mTLS) encrypts gRPC messages.

### **🔹 Step 1: Generate SSL Certificates**
```bash
openssl req -newkey rsa:2048 -nodes -keyout server-key.pem -x509 -days 365 -out server-cert.pem
openssl req -newkey rsa:2048 -nodes -keyout client-key.pem -x509 -days 365 -out client-cert.pem
```

✅ **Now, we have TLS certificates.**  

---

### **🔹 Step 2: Configure gRPC Server with TLS**
Modify `application.yml`:

```yaml
grpc:
  server:
    security:
      enabled: true
      certificate-chain: classpath:server-cert.pem
      private-key: classpath:server-key.pem
```

✅ **gRPC server now requires TLS.**  

---

### **🔹 Step 3: Configure gRPC Client with TLS**
Modify `inventory-service`:

```java
ManagedChannel channel = NettyChannelBuilder.forAddress("order-service", 9090)
    .sslContext(GrpcSslContexts.forClient()
        .trustManager(new File("server-cert.pem"))
        .build())
    .build();
```

✅ **Client now connects securely using TLS.** 🎉  

---

## **📌 Part 4: Combining Kafka & gRPC for Messaging**
1️⃣ **Use gRPC for real-time microservice-to-microservice communication.**  
2️⃣ **Use Kafka for asynchronous event-driven messaging.**  

### **🔹 Hybrid Architecture**
```
                     ┌──────────────────────┐
  Client  --->       │    Order Service     │  ---> Kafka (order-events)
                     └────────▲─────────────┘
                              │ gRPC
                     ┌────────▼──────────┐
                     │  Inventory Service │
                     └───────────────────┘
```

✅ **gRPC handles synchronous requests, Kafka ensures asynchronous messaging.**  

---

# **📌 Conclusion**
### **🚀 What We Achieved**
✅ **Secured Kafka using SASL, ACLs & OAuth.**  
✅ **Implemented gRPC for high-performance RPC.**  
✅ **Added Mutual TLS for secure gRPC communication.**  
✅ **Integrated Kafka & gRPC for efficient messaging.**  

🔹 **Next Steps:**  
- Implement **Kafka Stream Processing** for real-time analytics.  
- Use **Istio Service Mesh** for advanced security & observability.  


---

# **Implementing Kafka Stream Processing & Istio Service Mesh for Microservices**  

## **🚀 Introduction**  
As microservices scale, **real-time data processing and service management** become crucial. Kafka Streams enables **real-time analytics and transformation of event data**, while Istio provides **traffic control, security, and observability** for microservices.  

📌 **In this guide, we will:**  
✅ **Implement Kafka Stream Processing for real-time analytics.**  
✅ **Deploy Istio for traffic management and service security.**  
✅ **Use Istio for advanced observability with distributed tracing.**  
✅ **Enable JWT-based authentication and rate limiting with Istio.**  

---

# **📌 Part 1: Implementing Kafka Stream Processing**
Kafka Streams allows real-time processing **without requiring an external framework like Spark or Flink**.

## **🔹 Step 1: Add Kafka Streams Dependency**
In `pom.xml` of `analytics-service`:

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-streams</artifactId>
    <version>3.2.0</version>
</dependency>
```

✅ **This enables stream processing for Kafka topics.**  

---

## **🔹 Step 2: Create a Kafka Stream Processor**
📌 **We will process orders in real-time and compute total sales per product.**  

```java
@EnableKafkaStreams
@Configuration
public class KafkaStreamConfig {
    @Bean
    public KStream<String, OrderCreatedEvent> orderStream(StreamsBuilder builder) {
        KStream<String, OrderCreatedEvent> stream = builder.stream("order-events",
            Consumed.with(Serdes.String(), new OrderSerde()));

        stream.groupBy((key, order) -> order.getProduct())
              .aggregate(
                  () -> 0,
                  (key, order, total) -> total + order.getQuantity(),
                  Materialized.with(Serdes.String(), Serdes.Integer()))
              .toStream()
              .to("product-sales", Produced.with(Serdes.String(), Serdes.Integer()));

        return stream;
    }
}
```

✅ **Orders are grouped by product, and total sales are computed in real-time!** 🎉  

---

## **🔹 Step 3: Consume Processed Data in a New Service**
📌 **`reporting-service` will listen to the `product-sales` topic for live updates.**  

```java
@KafkaListener(topics = "product-sales", groupId = "reporting-group")
public void consumeSalesData(@Payload String message) {
    System.out.println("Updated sales data: " + message);
}
```

✅ **Now, the analytics system updates in real-time!** 🎉  

---

# **📌 Part 2: Deploying Istio Service Mesh**
Istio enables **traffic control, security policies, and observability**.

## **🔹 Step 1: Install Istio**
Run:

```bash
istioctl install --set profile=demo -y
```

✅ **Istio control plane is now installed.**  

---

## **🔹 Step 2: Deploy Microservices with Istio Sidecars**
Modify `kubernetes/deployment.yml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
spec:
  template:
    metadata:
      labels:
        app: order-service
    spec:
      containers:
        - name: order-service
          image: order-service:v1
          ports:
            - containerPort: 8081
      istio-proxy:
        enabled: true
```

✅ **Istio sidecars are injected, enabling service mesh capabilities.**  

---

## **🔹 Step 3: Enable Traffic Control in Istio**
📌 **Define a traffic rule to send 80% traffic to v1 and 20% to v2 of `order-service`.**  

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: order-service
spec:
  hosts:
    - order-service
  http:
    - route:
        - destination:
            host: order-service
            subset: v1
          weight: 80
        - destination:
            host: order-service
            subset: v2
          weight: 20
```

✅ **Traffic splitting is now enabled!** 🎉  

---

## **📌 Part 3: Implementing Security with Istio**
### **🔹 Step 1: Enable JWT Authentication**
📌 **All requests must include a valid JWT token.**

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: require-jwt
spec:
  selector:
    matchLabels:
      app: order-service
  action: DENY
  rules:
    - to:
        - operation:
            paths: ["/orders"]
      when:
        - key: request.auth.claims[role]
          values: ["user"]
```

✅ **Only authenticated users can access the API.**  

---

### **🔹 Step 2: Enable Rate Limiting with Istio**
📌 **Each client is limited to 5 requests per minute.**

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: rate-limit
spec:
  configPatches:
    - applyTo: HTTP_FILTER
      match:
        context: SIDECAR_INBOUND
      patch:
        operation: ADD
        value:
          name: envoy.rate_limit
          typed_config:
            "@type": type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimit
            domain: order-service
            rate_limit_service:
              grpc_service:
                envoy_grpc:
                  cluster_name: rate_limit
```

✅ **Rate limiting is now applied per client!** 🎉  

---

## **📌 Part 4: Enabling Observability with Istio**
### **🔹 Step 1: Enable Distributed Tracing**
Istio integrates with **Jaeger** for request tracing.

Modify `kubernetes/tracing.yml`:

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: Telemetry
metadata:
  name: tracing
spec:
  tracing:
    providers:
      - name: "jaeger"
    randomSamplingPercentage: 100
```

✅ **Requests are now traced end-to-end in Jaeger!** 🎉  

---

## **📌 Part 5: Testing the Setup**
### **🔹 Deploy Services to Kubernetes**
Run:

```bash
kubectl apply -f deployment.yml
kubectl apply -f virtualservice.yml
kubectl apply -f authorizationpolicy.yml
kubectl apply -f envoyfilter.yml
kubectl apply -f tracing.yml
```

✅ **Services are now deployed with Istio security and tracing enabled!** 🎉  

---

### **🔹 Generate Traffic for Testing**
Run:

```bash
for i in {1..10}; do curl http://order-service/orders; done
```

✅ **Jaeger UI (`http://localhost:16686`) now shows real-time traces!**  

---

# **📌 Conclusion**
### **🚀 What We Achieved**
✅ **Implemented real-time Kafka Stream Processing.**  
✅ **Deployed Istio for microservice security and traffic control.**  
✅ **Enabled JWT authentication and rate limiting in Istio.**  
✅ **Added distributed tracing with Jaeger.**  

🔹 **Next Steps:**  
- Implement **Kafka Streams for fraud detection in real-time.**  
- Use **Istio traffic mirroring for zero-downtime deployments.**  

---

# **Building a Real-Time Fraud Detection System with Kafka Streams**  

## **🚀 Introduction**  
Fraud detection is critical in **banking, e-commerce, and financial transactions**. By leveraging **Kafka Streams**, we can detect fraudulent transactions in **real-time** based on abnormal patterns.  

📌 **In this guide, we will:**  
✅ **Use Kafka Streams to process real-time transactions.**  
✅ **Apply anomaly detection rules for fraud detection.**  
✅ **Trigger alerts when fraudulent activity is detected.**  
✅ **Integrate with Istio for secure communication.**  

---

# **📌 Part 1: Setting Up Kafka Streams for Fraud Detection**
Kafka Streams enables **event-driven anomaly detection** without external batch processing systems.

## **🔹 Step 1: Define Fraud Detection Criteria**
Fraudulent transactions are detected based on:
- **High transaction amount** (e.g., > ₹10,00,000).
- **Multiple transactions from different locations within a short period**.
- **Transaction from a blocked account**.

---

## **🔹 Step 2: Add Kafka Streams Dependency**
Modify `pom.xml`:

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-streams</artifactId>
    <version>3.2.0</version>
</dependency>
```

✅ **Kafka Streams is now enabled.**  

---

## **📌 Part 2: Implementing Kafka Stream Processor for Fraud Detection**
📌 **Transactions are processed in real-time, and anomalies are flagged.**

### **🔹 Define Transaction Model**
```java
public class Transaction {
    private String transactionId;
    private String userId;
    private double amount;
    private String location;
    private long timestamp;
}
```

---

### **🔹 Implement Fraud Detection Logic**
```java
@EnableKafkaStreams
@Configuration
public class FraudDetectionStream {

    @Bean
    public KStream<String, Transaction> fraudDetectionStream(StreamsBuilder builder) {
        KStream<String, Transaction> stream = builder.stream("transactions",
            Consumed.with(Serdes.String(), new TransactionSerde()));

        // Detect transactions exceeding threshold
        stream.filter((key, transaction) -> transaction.getAmount() > 1000000)
              .to("fraud-alerts", Produced.with(Serdes.String(), new TransactionSerde()));

        return stream;
    }
}
```

✅ **Transactions with an amount > ₹10,00,000 are flagged as fraudulent.**  

---

## **📌 Part 3: Implement Advanced Fraud Detection (Multiple Location Checks)**
📌 **We check if a user made multiple transactions from different locations in a short time.**

### **🔹 Detect Multiple Locations**
```java
public static class TransactionAggregator {
    private String userId;
    private Set<String> locations = new HashSet<>();
    private long firstTransactionTime;
}
```

---

### **🔹 Implement Window-Based Fraud Detection**
```java
stream.groupByKey()
      .windowedBy(TimeWindows.of(Duration.ofMinutes(10)))
      .aggregate(
          TransactionAggregator::new,
          (key, transaction, aggregator) -> {
              aggregator.getLocations().add(transaction.getLocation());
              return aggregator;
          },
          Materialized.with(Serdes.String(), new TransactionAggregatorSerde()))
      .toStream()
      .filter((key, aggregator) -> aggregator.getLocations().size() > 2)
      .to("fraud-alerts", Produced.with(Serdes.String(), new TransactionAggregatorSerde()));
```

✅ **If a user transacts from 3+ locations within 10 minutes, an alert is triggered.** 🎉  

---

## **📌 Part 4: Consume Fraud Alerts and Trigger Action**
📌 **We listen to `fraud-alerts` topic and notify security teams.**

```java
@KafkaListener(topics = "fraud-alerts", groupId = "fraud-group")
public void consumeFraudAlert(Transaction transaction) {
    System.out.println("🚨 FRAUD DETECTED: " + transaction);
    alertService.notifySecurity(transaction);
}
```

✅ **Fraud alerts are now generated and sent to the security team.**  

---

# **📌 Part 5: Integrating Istio for Secure Fraud Detection**
Istio secures fraud detection microservices **via JWT authentication and rate limiting**.

## **🔹 Step 1: Require Authentication for Fraud APIs**
```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: require-jwt
spec:
  selector:
    matchLabels:
      app: fraud-service
  action: DENY
  rules:
    - to:
        - operation:
            methods: ["GET"]
      when:
        - key: request.auth.claims[role]
          values: ["admin"]
```

✅ **Only authenticated security admins can access fraud data.**  

---

## **🔹 Step 2: Enable Rate Limiting to Prevent API Abuse**
```yaml
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: rate-limit
spec:
  configPatches:
    - applyTo: HTTP_FILTER
      match:
        context: SIDECAR_INBOUND
      patch:
        operation: ADD
        value:
          name: envoy.rate_limit
          typed_config:
            "@type": type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimit
            domain: fraud-service
            rate_limit_service:
              grpc_service:
                envoy_grpc:
                  cluster_name: rate_limit
```

✅ **API rate limiting prevents abuse of the fraud detection system.**  

---

## **📌 Part 6: Visualizing Fraud Data in Grafana**
Fraud alerts are **logged into Prometheus**, and Grafana **visualizes the alerts**.

### **🔹 Configure Prometheus to Scrape Fraud Metrics**
Modify `prometheus.yml`:

```yaml
scrape_configs:
  - job_name: 'fraud-detection'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['fraud-service:8083']
```

✅ **Prometheus now tracks fraud metrics.**  

---

### **🔹 Create Grafana Dashboard for Fraud Alerts**
1. Open `http://localhost:3000`
2. Import **Prometheus as a Data Source**.
3. Add **custom visualization for fraud alerts**.

✅ **Fraud events are now visualized in real-time!** 🎉  

---

# **📌 Final Architecture**
```
                    ┌───────────────────────────┐
  User Transaction  →│   Kafka Transactions      │
                    └────────▲──────────────────┘
                             │
              ┌──────────────▼───────────────┐
              │ Fraud Detection Service       │
              │ - Detects anomalies           │
              │ - Publishes to fraud-alerts   │
              └───────────▲──────────────────┘
                          │
         ┌───────────────▼────────────────┐
         │     Kafka (fraud-alerts)       │
         └──────────────▲────────────────┘
                        │
     ┌─────────────────▼───────────────────┐
     │ Security Team Dashboard (Grafana)   │
     │ - Real-time alerts visualization    │
     │ - Investigates flagged transactions │
     └────────────────────────────────────┘
```

✅ **Fraud is detected and investigated in real-time!** 🎉  

---

# **📌 Conclusion**
### **🚀 What We Achieved**
✅ **Built a real-time fraud detection system using Kafka Streams.**  
✅ **Implemented window-based anomaly detection for transactions.**  
✅ **Integrated Istio for security (JWT & rate limiting).**  
✅ **Visualized fraud alerts in Grafana.**  

🔹 **Next Steps:**  
- Implement **Machine Learning for AI-driven fraud detection**.  
- Use **Kafka KSQL for SQL-based fraud analytics**.  

