# **Clean Architecture in Microservices: A Detailed Guide**  

## **🚀 Introduction**  
Microservices architecture has revolutionized software development by enabling **scalability, modularity, and independence** between services. However, poorly structured microservices can lead to **tightly coupled code, maintainability issues, and reduced testability**.  

**Clean Architecture** solves these problems by **separating concerns** and keeping business logic **independent of frameworks, databases, and external systems**.  

📌 **In this guide, we will cover:**  
✅ **What is Clean Architecture?**  
✅ **Core Principles of Clean Architecture**  
✅ **Layered Structure of Clean Architecture**  
✅ **How Clean Architecture Fits into Microservices**  
✅ **Hands-on Implementation in Spring Boot**  

---

# **📌 What is Clean Architecture?**  
Clean Architecture, proposed by **Robert C. Martin (Uncle Bob)**, is a **software design pattern** that ensures:  
- **Business logic remains independent** of databases, frameworks, and UI.  
- **High testability** by isolating dependencies.  
- **Loose coupling between components** for better scalability.  

### **🔹 The Four Layers of Clean Architecture**
```
                 +----------------------+
                 |    External Systems   |  → UI, Database, Frameworks, APIs
                 +----------------------+
                            │
                 +----------------------+
                 |       Adapters        |  → Controllers, Presenters, Gateways
                 +----------------------+
                            │
                 +----------------------+
                 |   Application Logic   |  → Use Cases, Business Rules
                 +----------------------+
                            │
                 +----------------------+
                 |    Domain Entities    |  → Core Business Models
                 +----------------------+
```

✅ **Core business logic is at the center** and does not depend on frameworks or external services.  

---

# **📌 Core Principles of Clean Architecture**
✅ **Dependency Inversion Principle** → Business logic should not depend on external frameworks.  
✅ **Separation of Concerns** → UI, application logic, and domain logic should be independent.  
✅ **Independence from Infrastructure** → Business logic must be independent of databases, APIs, or UI.  
✅ **Use Case Driven** → The system should be structured around use cases, not frameworks.  

---

# **📌 How Clean Architecture Fits into Microservices**
Microservices should follow **Clean Architecture** principles to ensure:  
- **Each microservice has its own clean structure**.  
- **Loose coupling** between services.  
- **Improved maintainability & testability**.  

---

# **📌 Hands-on Implementation in Spring Boot**
We will implement **Clean Architecture** for an **Order Management Microservice** using **Spring Boot**.  

## **🔹 Step 1: Define the Clean Architecture Layers**
📌 **Folder Structure:**
```
order-service/
│── src/
│   ├── main/java/com/example/order
│   │   ├── domain/         → (Entities, Business Rules)
│   │   ├── application/    → (Use Cases)
│   │   ├── adapters/       → (Controllers, Gateways)
│   │   ├── infrastructure/ → (Database, External APIs)
│── pom.xml
```
✅ **Each layer is independent, following Clean Architecture principles.**  

---

## **📌 Step 2: Define the Domain Layer**
The **Domain Layer** contains the **core business logic** and is independent of external dependencies.

### **🔹 Define `Order` Entity**
```java
package com.example.order.domain;

public class Order {
    private Long id;
    private String product;
    private int quantity;
    
    public Order(Long id, String product, int quantity) {
        this.id = id;
        this.product = product;
        this.quantity = quantity;
    }

    public Long getId() { return id; }
    public String getProduct() { return product; }
    public int getQuantity() { return quantity; }
}
```
✅ **This layer does NOT depend on Spring, database, or APIs.**  

---

## **📌 Step 3: Define the Application Layer**
The **Application Layer** contains **Use Cases** that coordinate domain logic.

### **🔹 Define `CreateOrderUseCase`**
```java
package com.example.order.application;

import com.example.order.domain.Order;
import com.example.order.ports.OrderRepositoryPort;

public class CreateOrderUseCase {
    private final OrderRepositoryPort orderRepository;

    public CreateOrderUseCase(OrderRepositoryPort orderRepository) {
        this.orderRepository = orderRepository;
    }

    public Order createOrder(Order order) {
        return orderRepository.save(order);
    }
}
```
✅ **The use case only interacts with an `OrderRepositoryPort`, NOT a database.**  

---

## **📌 Step 4: Define the Adapters Layer**
The **Adapters Layer** provides implementations for external dependencies like **databases and APIs**.

### **🔹 Define `OrderController` (Web Adapter)**
```java
package com.example.order.adapters;

import com.example.order.application.CreateOrderUseCase;
import com.example.order.domain.Order;
import org.springframework.web.bind.annotation.*;

@RestController
@RequestMapping("/orders")
public class OrderController {
    private final CreateOrderUseCase createOrderUseCase;

    public OrderController(CreateOrderUseCase createOrderUseCase) {
        this.createOrderUseCase = createOrderUseCase;
    }

    @PostMapping
    public Order createOrder(@RequestBody Order order) {
        return createOrderUseCase.createOrder(order);
    }
}
```
✅ **The controller only calls the Use Case, keeping business logic clean.**  

---

### **🔹 Define `OrderRepositoryAdapter` (Database Adapter)**
```java
package com.example.order.infrastructure;

import com.example.order.domain.Order;
import com.example.order.ports.OrderRepositoryPort;
import org.springframework.stereotype.Repository;

@Repository
public class OrderRepositoryAdapter implements OrderRepositoryPort {
    private final JpaOrderRepository jpaOrderRepository;

    public OrderRepositoryAdapter(JpaOrderRepository jpaOrderRepository) {
        this.jpaOrderRepository = jpaOrderRepository;
    }

    @Override
    public Order save(Order order) {
        return jpaOrderRepository.save(order);
    }
}
```
✅ **The business logic is NOT directly dependent on the database.**  

---

## **📌 Step 5: Define the Ports**
Ports act as **interfaces** between layers.

### **🔹 Define `OrderRepositoryPort` (Port for Database Access)**
```java
package com.example.order.ports;

import com.example.order.domain.Order;

public interface OrderRepositoryPort {
    Order save(Order order);
}
```
✅ **This abstraction ensures business logic does NOT depend on the database.**  

---

## **📌 Step 6: Connecting the Layers in `Main`**
```java
@SpringBootApplication
public class OrderServiceApplication {
    public static void main(String[] args) {
        SpringApplication.run(OrderServiceApplication.class, args);
    }
}
```

✅ **Spring Boot is only used in adapters and infrastructure!**  

---

# **📌 Benefits of Clean Architecture in Microservices**
✅ **Independent Business Logic** → Can be reused without framework changes.  
✅ **High Testability** → No database or API dependency in unit tests.  
✅ **Loose Coupling** → Changing the database, UI, or API does not affect business logic.  
✅ **Easier Maintenance** → Microservices remain scalable and modular.  

---

# **📌 Conclusion**
### **🚀 What We Achieved**
✅ **Designed a Clean Architecture for microservices.**  
✅ **Separated concerns across domain, application, and infrastructure.**  
✅ **Implemented Order Management Service in Spring Boot.**  
✅ **Enabled business logic independence and modularity.**  

🔹 **Next Steps:**  
- Implement **Event-Driven Architecture** with Kafka.  
- Add **GraphQL API instead of REST.**  

---

# **Integrating Event-Driven Architecture with Kafka in Clean Architecture**  

## **🚀 Introduction**  
Event-Driven Architecture (EDA) is essential in **microservices** to enable **asynchronous communication, scalability, and fault tolerance**. Combining **Kafka with Clean Architecture** ensures:  
- **Loose coupling between microservices**  
- **Scalable and reactive event processing**  
- **Independent and testable domain logic**  

📌 **In this guide, we will:**  
✅ **Implement Kafka-based event-driven architecture in Clean Architecture.**  
✅ **Ensure events flow asynchronously between microservices.**  
✅ **Keep business logic independent from Kafka using Ports & Adapters.**  
✅ **Enable reliable event handling and retry mechanisms.**  

---

# **📌 Part 1: Understanding Event-Driven Clean Architecture**
Clean Architecture ensures **business logic remains independent** of external dependencies like Kafka.

### **🔹 Updated Layered Structure with Kafka**
```
                 +----------------------+
                 |    External Systems   |  → Kafka, APIs, Databases
                 +----------------------+
                            │
                 +----------------------+
                 |       Adapters        |  → Producers, Consumers, Gateways
                 +----------------------+
                            │
                 +----------------------+
                 |   Application Logic   |  → Use Cases, Business Rules
                 +----------------------+
                            │
                 +----------------------+
                 |    Domain Entities    |  → Core Business Models
                 +----------------------+
```
✅ **Kafka acts as an external system, with events being handled through Ports & Adapters.**  

---

# **📌 Part 2: Implementing Kafka in Clean Architecture**
We will **modify an Order Management Service** to:  
1️⃣ **Publish an event when an order is created**.  
2️⃣ **Consume the event in another service (Inventory Service)**.  
3️⃣ **Ensure events remain decoupled from business logic**.  

---

## **📌 Step 1: Add Kafka Dependencies**
Modify `pom.xml` in **Order Service** and **Inventory Service**:

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

✅ **Kafka integration is enabled for both microservices.**  

---

## **📌 Step 2: Define the Domain Event Model**
📌 **Orders must be represented as events, separate from persistence models.**

```java
package com.example.order.domain.events;

public class OrderCreatedEvent {
    private Long orderId;
    private String product;
    private int quantity;

    public OrderCreatedEvent(Long orderId, String product, int quantity) {
        this.orderId = orderId;
        this.product = product;
        this.quantity = quantity;
    }

    public Long getOrderId() { return orderId; }
    public String getProduct() { return product; }
    public int getQuantity() { return quantity; }
}
```

✅ **Events are treated as separate domain entities.**  

---

## **📌 Step 3: Define the Kafka Port for Event Publishing**
📌 **Define an interface (`OrderEventPublisherPort`) in the `ports` layer to abstract Kafka usage.**

```java
package com.example.order.ports;

import com.example.order.domain.events.OrderCreatedEvent;

public interface OrderEventPublisherPort {
    void publishOrderCreatedEvent(OrderCreatedEvent event);
}
```

✅ **Business logic remains independent of Kafka!**  

---

## **📌 Step 4: Implement Kafka Producer (Adapter Layer)**
📌 **This class implements `OrderEventPublisherPort` and interacts with Kafka.**

```java
package com.example.order.adapters;

import com.example.order.domain.events.OrderCreatedEvent;
import com.example.order.ports.OrderEventPublisherPort;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

@Service
public class KafkaOrderEventPublisherAdapter implements OrderEventPublisherPort {
    private final KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate;

    public KafkaOrderEventPublisherAdapter(KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    @Override
    public void publishOrderCreatedEvent(OrderCreatedEvent event) {
        kafkaTemplate.send("order-events", event);
    }
}
```

✅ **Kafka remains outside the business logic, ensuring Clean Architecture principles!**  

---

## **📌 Step 5: Modify the Order Use Case to Publish Events**
📌 **The `CreateOrderUseCase` now calls the `OrderEventPublisherPort` after saving an order.**

```java
package com.example.order.application;

import com.example.order.domain.Order;
import com.example.order.domain.events.OrderCreatedEvent;
import com.example.order.ports.OrderEventPublisherPort;
import com.example.order.ports.OrderRepositoryPort;

public class CreateOrderUseCase {
    private final OrderRepositoryPort orderRepository;
    private final OrderEventPublisherPort eventPublisher;

    public CreateOrderUseCase(OrderRepositoryPort orderRepository, OrderEventPublisherPort eventPublisher) {
        this.orderRepository = orderRepository;
        this.eventPublisher = eventPublisher;
    }

    public Order createOrder(Order order) {
        Order savedOrder = orderRepository.save(order);
        eventPublisher.publishOrderCreatedEvent(new OrderCreatedEvent(savedOrder.getId(), savedOrder.getProduct(), savedOrder.getQuantity()));
        return savedOrder;
    }
}
```

✅ **The event is now published after saving an order!** 🎉  

---

## **📌 Step 6: Implement Kafka Consumer in Inventory Service**
📌 **The `InventoryService` listens for order events and updates stock.**  

### **🔹 Define the Kafka Consumer Adapter**
```java
package com.example.inventory.adapters;

import com.example.inventory.application.InventoryService;
import com.example.inventory.domain.events.OrderCreatedEvent;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class KafkaOrderEventConsumerAdapter {
    private final InventoryService inventoryService;

    public KafkaOrderEventConsumerAdapter(InventoryService inventoryService) {
        this.inventoryService = inventoryService;
    }

    @KafkaListener(topics = "order-events", groupId = "inventory-group")
    public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
        inventoryService.updateStock(event.getProduct(), event.getQuantity());
    }
}
```

✅ **Kafka Consumer is now an adapter, keeping the business logic clean.**  

---

## **📌 Step 7: Modify Inventory Service to Process Events**
```java
package com.example.inventory.application;

import org.springframework.stereotype.Service;

@Service
public class InventoryService {
    public void updateStock(String product, int quantity) {
        System.out.println("Updating inventory for product: " + product + ", quantity: " + quantity);
    }
}
```

✅ **Inventory updates happen automatically when an order event is received.**  

---

## **📌 Step 8: Configure Kafka in `application.yml`**
📌 **Both services need Kafka configurations for event processing.**  

**Order Service (`application.yml`)**  
```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
```

**Inventory Service (`application.yml`)**  
```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: inventory-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: "*"
```

✅ **Kafka is now configured for asynchronous event handling!**  

---

# **📌 Final Architecture**
```
                 ┌──────────────────────────┐
  Order Request →│ Order Service (Use Case) │
                 └───────────▲──────────────┘
                             │
                 ┌───────────▼──────────────┐
                 │ Kafka (order-events)     │
                 └───────────▲──────────────┘
                             │
                 ┌───────────▼──────────────┐
                 │ Inventory Service        │
                 │ - Listens for events     │
                 │ - Updates stock levels   │
                 └──────────────────────────┘
```

✅ **Orders and inventory updates are fully decoupled via Kafka!** 🎉  

---

# **📌 Conclusion**
### **🚀 What We Achieved**
✅ **Integrated Kafka into Clean Architecture using Ports & Adapters.**  
✅ **Published and consumed events asynchronously.**  
✅ **Ensured business logic remained independent from Kafka.**  
✅ **Created a fully decoupled microservices system.**  

🔹 **Next Steps:**  
- Implement **Kafka Schema Registry (Avro) for structured messaging.**  
- Add **Resilience patterns (Retries, Dead Letter Queues) for fault tolerance.**  

---

# **Implementing Kafka Schema Registry & Fault Tolerance in Event-Driven Microservices**  

## **🚀 Introduction**  
In an **event-driven microservices architecture**, ensuring **schema consistency and fault tolerance** is crucial.  
- **Kafka Schema Registry** helps **manage evolving schemas** for structured messages.  
- **Fault Tolerance Mechanisms** (Retries, Dead Letter Queues) ensure **reliable message delivery** even if consumers fail.  

📌 **In this guide, we will:**  
✅ **Use Avro with Kafka Schema Registry for structured messaging.**  
✅ **Implement Dead Letter Queues (DLQ) for handling failed messages.**  
✅ **Enable retry mechanisms for transient failures.**  

---

# **📌 Part 1: Implementing Kafka Schema Registry (Avro)**
### **🔹 Why Use Kafka Schema Registry?**
✅ **Prevents breaking changes when message structure evolves.**  
✅ **Ensures compatibility between producers and consumers.**  
✅ **Efficient binary serialization (smaller & faster than JSON).**  

---

## **📌 Step 1: Add Dependencies for Avro & Schema Registry**
Modify `pom.xml` in **Order Service** and **Inventory Service**:

```xml
<dependency>
    <groupId>io.confluent</groupId>
    <artifactId>kafka-avro-serializer</artifactId>
    <version>7.2.1</version>
</dependency>

<dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro</artifactId>
    <version>1.11.0</version>
</dependency>
```

✅ **This enables Avro serialization and Schema Registry integration.**  

---

## **📌 Step 2: Define Avro Schema for Order Events**
📌 **Define the Avro schema inside `src/main/resources/avro/order-event.avsc`:**  

```json
{
  "type": "record",
  "name": "OrderCreatedEvent",
  "namespace": "com.example.kafka",
  "fields": [
    { "name": "orderId", "type": "long" },
    { "name": "product", "type": "string" },
    { "name": "quantity", "type": "int" }
  ]
}
```

✅ **Defines the structured message format for Kafka.**  

---

## **📌 Step 3: Generate Java Classes from Avro Schema**
📌 **Use Maven to generate Java classes:**  

```xml
<plugin>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro-maven-plugin</artifactId>
    <version>1.11.0</version>
    <executions>
        <execution>
            <phase>generate-sources</phase>
            <goals>
                <goal>schema</goal>
            </goals>
            <configuration>
                <sourceDirectory>${project.basedir}/src/main/resources/avro</sourceDirectory>
                <outputDirectory>${project.build.directory}/generated-sources</outputDirectory>
            </configuration>
        </execution>
    </executions>
</plugin>
```

Run:
```bash
mvn clean install
```

✅ **Avro-generated Java classes are now available!** 🎉  

---

## **📌 Step 4: Configure Kafka Producer to Use Avro**
Modify `application.yml` in **Order Service**:

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    properties:
      schema.registry.url: http://localhost:8081
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
```

---

## **📌 Step 5: Publish Avro Messages in `OrderService`**
Modify **`KafkaOrderEventPublisherAdapter`**:

```java
@Service
public class KafkaOrderEventPublisherAdapter {
    private final KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate;

    @Autowired
    public KafkaOrderEventPublisherAdapter(KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    public void publishOrderCreatedEvent(Order order) {
        OrderCreatedEvent event = new OrderCreatedEvent(order.getId(), order.getProduct(), order.getQuantity());
        kafkaTemplate.send("order-events", event);
    }
}
```

✅ **Messages are now serialized in Avro format and stored in Schema Registry.**  

---

## **📌 Step 6: Configure Kafka Consumer to Use Avro**
Modify `application.yml` in **Inventory Service**:

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: inventory-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      properties:
        schema.registry.url: http://localhost:8081
        specific.avro.reader: true
```

---

## **📌 Step 7: Consume Avro Messages in `InventoryService`**
Modify **`KafkaOrderEventConsumerAdapter`**:

```java
@KafkaListener(topics = "order-events", groupId = "inventory-group")
public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
    System.out.println("Processing Avro message for Order ID: " + event.getOrderId());
    inventoryService.updateStock(event.getProduct(), event.getQuantity());
}
```

✅ **Kafka now ensures schema compatibility across services!** 🎉  

---

# **📌 Part 2: Implementing Fault Tolerance (Retries & Dead Letter Queue)**
To ensure **message reliability**, we will:  
1️⃣ **Retry transient failures (network/database issues).**  
2️⃣ **Send failed messages to a Dead Letter Queue (DLQ).**  

---

## **📌 Step 1: Configure Retry in Kafka Consumer**
Modify `application.yml` in **Inventory Service**:

```yaml
spring:
  kafka:
    consumer:
      max-poll-records: 1
      enable-auto-commit: false
      properties:
        retry.backoff.ms: 2000  # Retry interval
        max.poll.interval.ms: 60000
```

✅ **If a message processing fails, Kafka retries after 2 seconds.**  

---

## **📌 Step 2: Implement Retry in Consumer Code**
Modify **`KafkaOrderEventConsumerAdapter`**:

```java
@KafkaListener(topics = "order-events", groupId = "inventory-group")
@RetryableTopic(
    attempts = "3",
    backoff = @Backoff(delay = 2000))
public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
    if (event.getQuantity() < 0) {
        throw new RuntimeException("Invalid quantity detected!");
    }
    inventoryService.updateStock(event.getProduct(), event.getQuantity());
}
```

✅ **The consumer retries processing the message 3 times before failing.**  

---

## **📌 Step 3: Implement Dead Letter Queue (DLQ)**
📌 **Failed messages go to `order-events-dlq`.**  

Modify `application.yml`:

```yaml
spring:
  kafka:
    consumer:
      properties:
        dead-letter-topic: order-events-dlq
```

Modify **`KafkaOrderEventConsumerAdapter`**:

```java
@KafkaListener(topics = "order-events-dlq", groupId = "inventory-group")
public void handleFailedMessages(OrderCreatedEvent event) {
    System.out.println("🚨 Sending failed event to DLQ: " + event);
}
```

✅ **Now, failed messages are stored safely in `order-events-dlq`.** 🎉  

---

# **📌 Final Architecture**
```
                 ┌──────────────────────────┐
  Order Request →│ Order Service (Use Case) │
                 └───────────▲──────────────┘
                             │
                 ┌───────────▼──────────────┐
                 │ Kafka (order-events)     │
                 └───────────▲──────────────┘
                             │
                 ┌───────────▼──────────────┐
                 │ Inventory Service        │
                 │ - Listens for events     │
                 │ - Updates stock levels   │
                 └───────────▲──────────────┘
                             │
                 ┌───────────▼──────────────┐
                 │ Kafka (Dead Letter Queue)│
                 └──────────────────────────┘
```

✅ **Events are now structured (Avro), resilient (Retries), and fail-safe (DLQ)!** 🎉  

---

# **📌 Conclusion**
### **🚀 What We Achieved**
✅ **Implemented Kafka Schema Registry with Avro for structured messaging.**  
✅ **Enabled Retry Mechanism to handle transient failures.**  
✅ **Integrated Dead Letter Queue (DLQ) for failed messages.**  
✅ **Ensured schema compatibility across microservices.**  

🔹 **Next Steps:**  
- Implement **Kafka Streams for real-time analytics**.  
- Use **Kafka Connect for data integration**.  

---

# **Building Real-Time Analytics with Kafka Streams in Microservices**  

## **🚀 Introduction**  
Real-time analytics is essential for tracking system metrics, detecting anomalies, and generating insights **as events occur**. **Kafka Streams** is a lightweight **stream-processing framework** that enables microservices to **process, transform, and analyze data in real time**.  

📌 **In this guide, we will:**  
✅ **Implement Kafka Streams to process real-time order transactions.**  
✅ **Compute total sales per product using event aggregation.**  
✅ **Detect anomalies (e.g., high-value transactions) using windowing.**  
✅ **Visualize analytics data using Grafana & Prometheus.**  

---

# **📌 Part 1: Setting Up Kafka Streams**
## **🔹 Why Use Kafka Streams?**
✅ **Built-in scalability & fault tolerance**.  
✅ **Processes data in real time without batch jobs**.  
✅ **Supports **stateful** operations (e.g., aggregations, joins, windowing).**  
✅ **No need for additional dependencies like Spark or Flink.**  

---

## **📌 Step 1: Add Kafka Streams Dependencies**
Modify `pom.xml` in **Analytics Service**:

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-streams</artifactId>
    <version>3.2.0</version>
</dependency>

<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

✅ **Kafka Streams is now enabled.**  

---

## **📌 Step 2: Define the Order Event Model**
📌 **Orders must be represented as events for real-time processing.**  

```java
package com.example.analytics.domain;

public class OrderEvent {
    private String orderId;
    private String product;
    private int quantity;
    private double price;

    public OrderEvent(String orderId, String product, int quantity, double price) {
        this.orderId = orderId;
        this.product = product;
        this.quantity = quantity;
        this.price = price;
    }

    public String getProduct() { return product; }
    public int getQuantity() { return quantity; }
    public double getPrice() { return price; }
}
```

✅ **Each order contains product details, quantity, and price.**  

---

# **📌 Part 2: Implementing Kafka Streams for Real-Time Aggregation**
We will compute **total sales per product** using Kafka Streams.

## **🔹 Step 1: Implement Sales Aggregation**
📌 **The analytics service listens for `order-events`, aggregates sales, and stores results in `sales-summary`.**

```java
@EnableKafkaStreams
@Configuration
public class SalesAggregationStream {

    @Bean
    public KStream<String, OrderEvent> orderStream(StreamsBuilder builder) {
        KStream<String, OrderEvent> stream = builder.stream("order-events",
            Consumed.with(Serdes.String(), new OrderEventSerde()));

        stream.groupBy((key, order) -> order.getProduct())
              .aggregate(
                  () -> 0.0,
                  (key, order, totalSales) -> totalSales + (order.getPrice() * order.getQuantity()),
                  Materialized.with(Serdes.String(), Serdes.Double()))
              .toStream()
              .to("sales-summary", Produced.with(Serdes.String(), Serdes.Double()));

        return stream;
    }
}
```

✅ **Kafka Streams continuously updates total sales per product.** 🎉  

---

## **📌 Step 2: Consume Aggregated Data in Reporting Service**
📌 **The reporting service listens for `sales-summary` to generate reports.**

```java
@KafkaListener(topics = "sales-summary", groupId = "reporting-group")
public void consumeSalesData(@Payload String message) {
    System.out.println("Updated sales data: " + message);
}
```

✅ **Sales insights are now available in real-time.** 🎉  

---

# **📌 Part 3: Detecting Anomalies with Windowing**
## **🔹 Why Use Windowing?**
✅ **Detect fraud by checking multiple transactions in short intervals.**  
✅ **Identify sudden sales spikes for promotions.**  

### **📌 Implement Window-Based Fraud Detection**
📌 **Detect if a user places orders exceeding ₹10,00,000 within 10 minutes.**  

```java
public static class TransactionAggregator {
    private double totalAmount = 0.0;
}

stream.groupBy((key, order) -> order.getOrderId())
      .windowedBy(TimeWindows.of(Duration.ofMinutes(10)))
      .aggregate(
          TransactionAggregator::new,
          (key, order, aggregator) -> {
              aggregator.totalAmount += order.getPrice() * order.getQuantity();
              return aggregator;
          },
          Materialized.with(Serdes.String(), new TransactionAggregatorSerde()))
      .toStream()
      .filter((key, aggregator) -> aggregator.totalAmount > 1000000)
      .to("fraud-alerts", Produced.with(Serdes.String(), new TransactionAggregatorSerde()));
```

✅ **Fraudulent activity is detected and sent to the `fraud-alerts` topic!** 🎉  

---

# **📌 Part 4: Visualizing Real-Time Analytics with Grafana**
📌 **We will store analytics data in Prometheus and visualize it using Grafana.**  

## **🔹 Step 1: Configure Prometheus to Scrape Metrics**
Modify `prometheus.yml`:

```yaml
scrape_configs:
  - job_name: 'kafka-metrics'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['analytics-service:8085']
```

✅ **Prometheus now scrapes Kafka Streams metrics.**  

---

## **🔹 Step 2: Create a Grafana Dashboard**
1️⃣ Open **Grafana (`http://localhost:3000`)**  
2️⃣ Add **Prometheus as a Data Source**  
3️⃣ Create a new **Dashboard**  
4️⃣ Add **Real-time Sales Visualization**  
```yaml
query: sum(rate(kafka_sales_summary_total[5m]))
```

✅ **Real-time sales insights are now displayed in Grafana!** 🎉  

---

# **📌 Final Architecture**
```
                  ┌──────────────────────────┐
  Orders  ----->  │ Kafka (order-events)     │
                  └───────────▲──────────────┘
                              │
                  ┌───────────▼──────────────┐
                  │ Sales Aggregation Stream │
                  │ - Computes total sales   │
                  └───────────▲──────────────┘
                              │
                  ┌───────────▼──────────────┐
                  │ Kafka (sales-summary)    │
                  └───────────▲──────────────┘
                              │
    ┌────────────────────────▼────────────────────────┐
    │ Reporting Service (Grafana + Prometheus)        │
    │ - Visualizes sales trends                       │
    │ - Detects anomalies                             │
    └────────────────────────────────────────────────┘
```

✅ **Real-time analytics is now fully operational!** 🎉  

---

# **📌 Conclusion**
### **🚀 What We Achieved**
✅ **Implemented Kafka Streams for real-time order processing.**  
✅ **Computed total sales per product using aggregation.**  
✅ **Detected anomalies using time-windowed fraud detection.**  
✅ **Visualized real-time analytics with Grafana & Prometheus.**  

🔹 **Next Steps:**  
- Implement **Machine Learning for predictive analytics**.  
- Use **Kafka KSQL for real-time SQL-based querying**.  

> Keep exploring... :)