# **Clean Architecture in Microservices: A Detailed Guide**  

## **ğŸš€ Introduction**  
Microservices architecture has revolutionized software development by enabling **scalability, modularity, and independence** between services. However, poorly structured microservices can lead to **tightly coupled code, maintainability issues, and reduced testability**.  

**Clean Architecture** solves these problems by **separating concerns** and keeping business logic **independent of frameworks, databases, and external systems**.  

ğŸ“Œ **In this guide, we will cover:**  
âœ… **What is Clean Architecture?**  
âœ… **Core Principles of Clean Architecture**  
âœ… **Layered Structure of Clean Architecture**  
âœ… **How Clean Architecture Fits into Microservices**  
âœ… **Hands-on Implementation in Spring Boot**  

---

# **ğŸ“Œ What is Clean Architecture?**  
Clean Architecture, proposed by **Robert C. Martin (Uncle Bob)**, is a **software design pattern** that ensures:  
- **Business logic remains independent** of databases, frameworks, and UI.  
- **High testability** by isolating dependencies.  
- **Loose coupling between components** for better scalability.  

### **ğŸ”¹ The Four Layers of Clean Architecture**
```
                 +----------------------+
                 |    External Systems   |  â†’ UI, Database, Frameworks, APIs
                 +----------------------+
                            â”‚
                 +----------------------+
                 |       Adapters        |  â†’ Controllers, Presenters, Gateways
                 +----------------------+
                            â”‚
                 +----------------------+
                 |   Application Logic   |  â†’ Use Cases, Business Rules
                 +----------------------+
                            â”‚
                 +----------------------+
                 |    Domain Entities    |  â†’ Core Business Models
                 +----------------------+
```

âœ… **Core business logic is at the center** and does not depend on frameworks or external services.  

---

# **ğŸ“Œ Core Principles of Clean Architecture**
âœ… **Dependency Inversion Principle** â†’ Business logic should not depend on external frameworks.  
âœ… **Separation of Concerns** â†’ UI, application logic, and domain logic should be independent.  
âœ… **Independence from Infrastructure** â†’ Business logic must be independent of databases, APIs, or UI.  
âœ… **Use Case Driven** â†’ The system should be structured around use cases, not frameworks.  

---

# **ğŸ“Œ How Clean Architecture Fits into Microservices**
Microservices should follow **Clean Architecture** principles to ensure:  
- **Each microservice has its own clean structure**.  
- **Loose coupling** between services.  
- **Improved maintainability & testability**.  

---

# **ğŸ“Œ Hands-on Implementation in Spring Boot**
We will implement **Clean Architecture** for an **Order Management Microservice** using **Spring Boot**.  

## **ğŸ”¹ Step 1: Define the Clean Architecture Layers**
ğŸ“Œ **Folder Structure:**
```
order-service/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ main/java/com/example/order
â”‚   â”‚   â”œâ”€â”€ domain/         â†’ (Entities, Business Rules)
â”‚   â”‚   â”œâ”€â”€ application/    â†’ (Use Cases)
â”‚   â”‚   â”œâ”€â”€ adapters/       â†’ (Controllers, Gateways)
â”‚   â”‚   â”œâ”€â”€ infrastructure/ â†’ (Database, External APIs)
â”‚â”€â”€ pom.xml
```
âœ… **Each layer is independent, following Clean Architecture principles.**  

---

## **ğŸ“Œ Step 2: Define the Domain Layer**
The **Domain Layer** contains the **core business logic** and is independent of external dependencies.

### **ğŸ”¹ Define `Order` Entity**
```java
package com.example.order.domain;

public class Order {
    private Long id;
    private String product;
    private int quantity;
    
    public Order(Long id, String product, int quantity) {
        this.id = id;
        this.product = product;
        this.quantity = quantity;
    }

    public Long getId() { return id; }
    public String getProduct() { return product; }
    public int getQuantity() { return quantity; }
}
```
âœ… **This layer does NOT depend on Spring, database, or APIs.**  

---

## **ğŸ“Œ Step 3: Define the Application Layer**
The **Application Layer** contains **Use Cases** that coordinate domain logic.

### **ğŸ”¹ Define `CreateOrderUseCase`**
```java
package com.example.order.application;

import com.example.order.domain.Order;
import com.example.order.ports.OrderRepositoryPort;

public class CreateOrderUseCase {
    private final OrderRepositoryPort orderRepository;

    public CreateOrderUseCase(OrderRepositoryPort orderRepository) {
        this.orderRepository = orderRepository;
    }

    public Order createOrder(Order order) {
        return orderRepository.save(order);
    }
}
```
âœ… **The use case only interacts with an `OrderRepositoryPort`, NOT a database.**  

---

## **ğŸ“Œ Step 4: Define the Adapters Layer**
The **Adapters Layer** provides implementations for external dependencies like **databases and APIs**.

### **ğŸ”¹ Define `OrderController` (Web Adapter)**
```java
package com.example.order.adapters;

import com.example.order.application.CreateOrderUseCase;
import com.example.order.domain.Order;
import org.springframework.web.bind.annotation.*;

@RestController
@RequestMapping("/orders")
public class OrderController {
    private final CreateOrderUseCase createOrderUseCase;

    public OrderController(CreateOrderUseCase createOrderUseCase) {
        this.createOrderUseCase = createOrderUseCase;
    }

    @PostMapping
    public Order createOrder(@RequestBody Order order) {
        return createOrderUseCase.createOrder(order);
    }
}
```
âœ… **The controller only calls the Use Case, keeping business logic clean.**  

---

### **ğŸ”¹ Define `OrderRepositoryAdapter` (Database Adapter)**
```java
package com.example.order.infrastructure;

import com.example.order.domain.Order;
import com.example.order.ports.OrderRepositoryPort;
import org.springframework.stereotype.Repository;

@Repository
public class OrderRepositoryAdapter implements OrderRepositoryPort {
    private final JpaOrderRepository jpaOrderRepository;

    public OrderRepositoryAdapter(JpaOrderRepository jpaOrderRepository) {
        this.jpaOrderRepository = jpaOrderRepository;
    }

    @Override
    public Order save(Order order) {
        return jpaOrderRepository.save(order);
    }
}
```
âœ… **The business logic is NOT directly dependent on the database.**  

---

## **ğŸ“Œ Step 5: Define the Ports**
Ports act as **interfaces** between layers.

### **ğŸ”¹ Define `OrderRepositoryPort` (Port for Database Access)**
```java
package com.example.order.ports;

import com.example.order.domain.Order;

public interface OrderRepositoryPort {
    Order save(Order order);
}
```
âœ… **This abstraction ensures business logic does NOT depend on the database.**  

---

## **ğŸ“Œ Step 6: Connecting the Layers in `Main`**
```java
@SpringBootApplication
public class OrderServiceApplication {
    public static void main(String[] args) {
        SpringApplication.run(OrderServiceApplication.class, args);
    }
}
```

âœ… **Spring Boot is only used in adapters and infrastructure!**  

---

# **ğŸ“Œ Benefits of Clean Architecture in Microservices**
âœ… **Independent Business Logic** â†’ Can be reused without framework changes.  
âœ… **High Testability** â†’ No database or API dependency in unit tests.  
âœ… **Loose Coupling** â†’ Changing the database, UI, or API does not affect business logic.  
âœ… **Easier Maintenance** â†’ Microservices remain scalable and modular.  

---

# **ğŸ“Œ Conclusion**
### **ğŸš€ What We Achieved**
âœ… **Designed a Clean Architecture for microservices.**  
âœ… **Separated concerns across domain, application, and infrastructure.**  
âœ… **Implemented Order Management Service in Spring Boot.**  
âœ… **Enabled business logic independence and modularity.**  

ğŸ”¹ **Next Steps:**  
- Implement **Event-Driven Architecture** with Kafka.  
- Add **GraphQL API instead of REST.**  

---

# **Integrating Event-Driven Architecture with Kafka in Clean Architecture**  

## **ğŸš€ Introduction**  
Event-Driven Architecture (EDA) is essential in **microservices** to enable **asynchronous communication, scalability, and fault tolerance**. Combining **Kafka with Clean Architecture** ensures:  
- **Loose coupling between microservices**  
- **Scalable and reactive event processing**  
- **Independent and testable domain logic**  

ğŸ“Œ **In this guide, we will:**  
âœ… **Implement Kafka-based event-driven architecture in Clean Architecture.**  
âœ… **Ensure events flow asynchronously between microservices.**  
âœ… **Keep business logic independent from Kafka using Ports & Adapters.**  
âœ… **Enable reliable event handling and retry mechanisms.**  

---

# **ğŸ“Œ Part 1: Understanding Event-Driven Clean Architecture**
Clean Architecture ensures **business logic remains independent** of external dependencies like Kafka.

### **ğŸ”¹ Updated Layered Structure with Kafka**
```
                 +----------------------+
                 |    External Systems   |  â†’ Kafka, APIs, Databases
                 +----------------------+
                            â”‚
                 +----------------------+
                 |       Adapters        |  â†’ Producers, Consumers, Gateways
                 +----------------------+
                            â”‚
                 +----------------------+
                 |   Application Logic   |  â†’ Use Cases, Business Rules
                 +----------------------+
                            â”‚
                 +----------------------+
                 |    Domain Entities    |  â†’ Core Business Models
                 +----------------------+
```
âœ… **Kafka acts as an external system, with events being handled through Ports & Adapters.**  

---

# **ğŸ“Œ Part 2: Implementing Kafka in Clean Architecture**
We will **modify an Order Management Service** to:  
1ï¸âƒ£ **Publish an event when an order is created**.  
2ï¸âƒ£ **Consume the event in another service (Inventory Service)**.  
3ï¸âƒ£ **Ensure events remain decoupled from business logic**.  

---

## **ğŸ“Œ Step 1: Add Kafka Dependencies**
Modify `pom.xml` in **Order Service** and **Inventory Service**:

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

âœ… **Kafka integration is enabled for both microservices.**  

---

## **ğŸ“Œ Step 2: Define the Domain Event Model**
ğŸ“Œ **Orders must be represented as events, separate from persistence models.**

```java
package com.example.order.domain.events;

public class OrderCreatedEvent {
    private Long orderId;
    private String product;
    private int quantity;

    public OrderCreatedEvent(Long orderId, String product, int quantity) {
        this.orderId = orderId;
        this.product = product;
        this.quantity = quantity;
    }

    public Long getOrderId() { return orderId; }
    public String getProduct() { return product; }
    public int getQuantity() { return quantity; }
}
```

âœ… **Events are treated as separate domain entities.**  

---

## **ğŸ“Œ Step 3: Define the Kafka Port for Event Publishing**
ğŸ“Œ **Define an interface (`OrderEventPublisherPort`) in the `ports` layer to abstract Kafka usage.**

```java
package com.example.order.ports;

import com.example.order.domain.events.OrderCreatedEvent;

public interface OrderEventPublisherPort {
    void publishOrderCreatedEvent(OrderCreatedEvent event);
}
```

âœ… **Business logic remains independent of Kafka!**  

---

## **ğŸ“Œ Step 4: Implement Kafka Producer (Adapter Layer)**
ğŸ“Œ **This class implements `OrderEventPublisherPort` and interacts with Kafka.**

```java
package com.example.order.adapters;

import com.example.order.domain.events.OrderCreatedEvent;
import com.example.order.ports.OrderEventPublisherPort;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

@Service
public class KafkaOrderEventPublisherAdapter implements OrderEventPublisherPort {
    private final KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate;

    public KafkaOrderEventPublisherAdapter(KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    @Override
    public void publishOrderCreatedEvent(OrderCreatedEvent event) {
        kafkaTemplate.send("order-events", event);
    }
}
```

âœ… **Kafka remains outside the business logic, ensuring Clean Architecture principles!**  

---

## **ğŸ“Œ Step 5: Modify the Order Use Case to Publish Events**
ğŸ“Œ **The `CreateOrderUseCase` now calls the `OrderEventPublisherPort` after saving an order.**

```java
package com.example.order.application;

import com.example.order.domain.Order;
import com.example.order.domain.events.OrderCreatedEvent;
import com.example.order.ports.OrderEventPublisherPort;
import com.example.order.ports.OrderRepositoryPort;

public class CreateOrderUseCase {
    private final OrderRepositoryPort orderRepository;
    private final OrderEventPublisherPort eventPublisher;

    public CreateOrderUseCase(OrderRepositoryPort orderRepository, OrderEventPublisherPort eventPublisher) {
        this.orderRepository = orderRepository;
        this.eventPublisher = eventPublisher;
    }

    public Order createOrder(Order order) {
        Order savedOrder = orderRepository.save(order);
        eventPublisher.publishOrderCreatedEvent(new OrderCreatedEvent(savedOrder.getId(), savedOrder.getProduct(), savedOrder.getQuantity()));
        return savedOrder;
    }
}
```

âœ… **The event is now published after saving an order!** ğŸ‰  

---

## **ğŸ“Œ Step 6: Implement Kafka Consumer in Inventory Service**
ğŸ“Œ **The `InventoryService` listens for order events and updates stock.**  

### **ğŸ”¹ Define the Kafka Consumer Adapter**
```java
package com.example.inventory.adapters;

import com.example.inventory.application.InventoryService;
import com.example.inventory.domain.events.OrderCreatedEvent;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class KafkaOrderEventConsumerAdapter {
    private final InventoryService inventoryService;

    public KafkaOrderEventConsumerAdapter(InventoryService inventoryService) {
        this.inventoryService = inventoryService;
    }

    @KafkaListener(topics = "order-events", groupId = "inventory-group")
    public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
        inventoryService.updateStock(event.getProduct(), event.getQuantity());
    }
}
```

âœ… **Kafka Consumer is now an adapter, keeping the business logic clean.**  

---

## **ğŸ“Œ Step 7: Modify Inventory Service to Process Events**
```java
package com.example.inventory.application;

import org.springframework.stereotype.Service;

@Service
public class InventoryService {
    public void updateStock(String product, int quantity) {
        System.out.println("Updating inventory for product: " + product + ", quantity: " + quantity);
    }
}
```

âœ… **Inventory updates happen automatically when an order event is received.**  

---

## **ğŸ“Œ Step 8: Configure Kafka in `application.yml`**
ğŸ“Œ **Both services need Kafka configurations for event processing.**  

**Order Service (`application.yml`)**  
```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
```

**Inventory Service (`application.yml`)**  
```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: inventory-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: "*"
```

âœ… **Kafka is now configured for asynchronous event handling!**  

---

# **ğŸ“Œ Final Architecture**
```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  Order Request â†’â”‚ Order Service (Use Case) â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Kafka (order-events)     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Inventory Service        â”‚
                 â”‚ - Listens for events     â”‚
                 â”‚ - Updates stock levels   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

âœ… **Orders and inventory updates are fully decoupled via Kafka!** ğŸ‰  

---

# **ğŸ“Œ Conclusion**
### **ğŸš€ What We Achieved**
âœ… **Integrated Kafka into Clean Architecture using Ports & Adapters.**  
âœ… **Published and consumed events asynchronously.**  
âœ… **Ensured business logic remained independent from Kafka.**  
âœ… **Created a fully decoupled microservices system.**  

ğŸ”¹ **Next Steps:**  
- Implement **Kafka Schema Registry (Avro) for structured messaging.**  
- Add **Resilience patterns (Retries, Dead Letter Queues) for fault tolerance.**  

---

# **Implementing Kafka Schema Registry & Fault Tolerance in Event-Driven Microservices**  

## **ğŸš€ Introduction**  
In an **event-driven microservices architecture**, ensuring **schema consistency and fault tolerance** is crucial.  
- **Kafka Schema Registry** helps **manage evolving schemas** for structured messages.  
- **Fault Tolerance Mechanisms** (Retries, Dead Letter Queues) ensure **reliable message delivery** even if consumers fail.  

ğŸ“Œ **In this guide, we will:**  
âœ… **Use Avro with Kafka Schema Registry for structured messaging.**  
âœ… **Implement Dead Letter Queues (DLQ) for handling failed messages.**  
âœ… **Enable retry mechanisms for transient failures.**  

---

# **ğŸ“Œ Part 1: Implementing Kafka Schema Registry (Avro)**
### **ğŸ”¹ Why Use Kafka Schema Registry?**
âœ… **Prevents breaking changes when message structure evolves.**  
âœ… **Ensures compatibility between producers and consumers.**  
âœ… **Efficient binary serialization (smaller & faster than JSON).**  

---

## **ğŸ“Œ Step 1: Add Dependencies for Avro & Schema Registry**
Modify `pom.xml` in **Order Service** and **Inventory Service**:

```xml
<dependency>
    <groupId>io.confluent</groupId>
    <artifactId>kafka-avro-serializer</artifactId>
    <version>7.2.1</version>
</dependency>

<dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro</artifactId>
    <version>1.11.0</version>
</dependency>
```

âœ… **This enables Avro serialization and Schema Registry integration.**  

---

## **ğŸ“Œ Step 2: Define Avro Schema for Order Events**
ğŸ“Œ **Define the Avro schema inside `src/main/resources/avro/order-event.avsc`:**  

```json
{
  "type": "record",
  "name": "OrderCreatedEvent",
  "namespace": "com.example.kafka",
  "fields": [
    { "name": "orderId", "type": "long" },
    { "name": "product", "type": "string" },
    { "name": "quantity", "type": "int" }
  ]
}
```

âœ… **Defines the structured message format for Kafka.**  

---

## **ğŸ“Œ Step 3: Generate Java Classes from Avro Schema**
ğŸ“Œ **Use Maven to generate Java classes:**  

```xml
<plugin>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro-maven-plugin</artifactId>
    <version>1.11.0</version>
    <executions>
        <execution>
            <phase>generate-sources</phase>
            <goals>
                <goal>schema</goal>
            </goals>
            <configuration>
                <sourceDirectory>${project.basedir}/src/main/resources/avro</sourceDirectory>
                <outputDirectory>${project.build.directory}/generated-sources</outputDirectory>
            </configuration>
        </execution>
    </executions>
</plugin>
```

Run:
```bash
mvn clean install
```

âœ… **Avro-generated Java classes are now available!** ğŸ‰  

---

## **ğŸ“Œ Step 4: Configure Kafka Producer to Use Avro**
Modify `application.yml` in **Order Service**:

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    properties:
      schema.registry.url: http://localhost:8081
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
```

---

## **ğŸ“Œ Step 5: Publish Avro Messages in `OrderService`**
Modify **`KafkaOrderEventPublisherAdapter`**:

```java
@Service
public class KafkaOrderEventPublisherAdapter {
    private final KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate;

    @Autowired
    public KafkaOrderEventPublisherAdapter(KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }

    public void publishOrderCreatedEvent(Order order) {
        OrderCreatedEvent event = new OrderCreatedEvent(order.getId(), order.getProduct(), order.getQuantity());
        kafkaTemplate.send("order-events", event);
    }
}
```

âœ… **Messages are now serialized in Avro format and stored in Schema Registry.**  

---

## **ğŸ“Œ Step 6: Configure Kafka Consumer to Use Avro**
Modify `application.yml` in **Inventory Service**:

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: inventory-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      properties:
        schema.registry.url: http://localhost:8081
        specific.avro.reader: true
```

---

## **ğŸ“Œ Step 7: Consume Avro Messages in `InventoryService`**
Modify **`KafkaOrderEventConsumerAdapter`**:

```java
@KafkaListener(topics = "order-events", groupId = "inventory-group")
public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
    System.out.println("Processing Avro message for Order ID: " + event.getOrderId());
    inventoryService.updateStock(event.getProduct(), event.getQuantity());
}
```

âœ… **Kafka now ensures schema compatibility across services!** ğŸ‰  

---

# **ğŸ“Œ Part 2: Implementing Fault Tolerance (Retries & Dead Letter Queue)**
To ensure **message reliability**, we will:  
1ï¸âƒ£ **Retry transient failures (network/database issues).**  
2ï¸âƒ£ **Send failed messages to a Dead Letter Queue (DLQ).**  

---

## **ğŸ“Œ Step 1: Configure Retry in Kafka Consumer**
Modify `application.yml` in **Inventory Service**:

```yaml
spring:
  kafka:
    consumer:
      max-poll-records: 1
      enable-auto-commit: false
      properties:
        retry.backoff.ms: 2000  # Retry interval
        max.poll.interval.ms: 60000
```

âœ… **If a message processing fails, Kafka retries after 2 seconds.**  

---

## **ğŸ“Œ Step 2: Implement Retry in Consumer Code**
Modify **`KafkaOrderEventConsumerAdapter`**:

```java
@KafkaListener(topics = "order-events", groupId = "inventory-group")
@RetryableTopic(
    attempts = "3",
    backoff = @Backoff(delay = 2000))
public void consumeOrderCreatedEvent(OrderCreatedEvent event) {
    if (event.getQuantity() < 0) {
        throw new RuntimeException("Invalid quantity detected!");
    }
    inventoryService.updateStock(event.getProduct(), event.getQuantity());
}
```

âœ… **The consumer retries processing the message 3 times before failing.**  

---

## **ğŸ“Œ Step 3: Implement Dead Letter Queue (DLQ)**
ğŸ“Œ **Failed messages go to `order-events-dlq`.**  

Modify `application.yml`:

```yaml
spring:
  kafka:
    consumer:
      properties:
        dead-letter-topic: order-events-dlq
```

Modify **`KafkaOrderEventConsumerAdapter`**:

```java
@KafkaListener(topics = "order-events-dlq", groupId = "inventory-group")
public void handleFailedMessages(OrderCreatedEvent event) {
    System.out.println("ğŸš¨ Sending failed event to DLQ: " + event);
}
```

âœ… **Now, failed messages are stored safely in `order-events-dlq`.** ğŸ‰  

---

# **ğŸ“Œ Final Architecture**
```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  Order Request â†’â”‚ Order Service (Use Case) â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Kafka (order-events)     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Inventory Service        â”‚
                 â”‚ - Listens for events     â”‚
                 â”‚ - Updates stock levels   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Kafka (Dead Letter Queue)â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

âœ… **Events are now structured (Avro), resilient (Retries), and fail-safe (DLQ)!** ğŸ‰  

---

# **ğŸ“Œ Conclusion**
### **ğŸš€ What We Achieved**
âœ… **Implemented Kafka Schema Registry with Avro for structured messaging.**  
âœ… **Enabled Retry Mechanism to handle transient failures.**  
âœ… **Integrated Dead Letter Queue (DLQ) for failed messages.**  
âœ… **Ensured schema compatibility across microservices.**  

ğŸ”¹ **Next Steps:**  
- Implement **Kafka Streams for real-time analytics**.  
- Use **Kafka Connect for data integration**.  

---

# **Building Real-Time Analytics with Kafka Streams in Microservices**  

## **ğŸš€ Introduction**  
Real-time analytics is essential for tracking system metrics, detecting anomalies, and generating insights **as events occur**. **Kafka Streams** is a lightweight **stream-processing framework** that enables microservices to **process, transform, and analyze data in real time**.  

ğŸ“Œ **In this guide, we will:**  
âœ… **Implement Kafka Streams to process real-time order transactions.**  
âœ… **Compute total sales per product using event aggregation.**  
âœ… **Detect anomalies (e.g., high-value transactions) using windowing.**  
âœ… **Visualize analytics data using Grafana & Prometheus.**  

---

# **ğŸ“Œ Part 1: Setting Up Kafka Streams**
## **ğŸ”¹ Why Use Kafka Streams?**
âœ… **Built-in scalability & fault tolerance**.  
âœ… **Processes data in real time without batch jobs**.  
âœ… **Supports **stateful** operations (e.g., aggregations, joins, windowing).**  
âœ… **No need for additional dependencies like Spark or Flink.**  

---

## **ğŸ“Œ Step 1: Add Kafka Streams Dependencies**
Modify `pom.xml` in **Analytics Service**:

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-streams</artifactId>
    <version>3.2.0</version>
</dependency>

<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

âœ… **Kafka Streams is now enabled.**  

---

## **ğŸ“Œ Step 2: Define the Order Event Model**
ğŸ“Œ **Orders must be represented as events for real-time processing.**  

```java
package com.example.analytics.domain;

public class OrderEvent {
    private String orderId;
    private String product;
    private int quantity;
    private double price;

    public OrderEvent(String orderId, String product, int quantity, double price) {
        this.orderId = orderId;
        this.product = product;
        this.quantity = quantity;
        this.price = price;
    }

    public String getProduct() { return product; }
    public int getQuantity() { return quantity; }
    public double getPrice() { return price; }
}
```

âœ… **Each order contains product details, quantity, and price.**  

---

# **ğŸ“Œ Part 2: Implementing Kafka Streams for Real-Time Aggregation**
We will compute **total sales per product** using Kafka Streams.

## **ğŸ”¹ Step 1: Implement Sales Aggregation**
ğŸ“Œ **The analytics service listens for `order-events`, aggregates sales, and stores results in `sales-summary`.**

```java
@EnableKafkaStreams
@Configuration
public class SalesAggregationStream {

    @Bean
    public KStream<String, OrderEvent> orderStream(StreamsBuilder builder) {
        KStream<String, OrderEvent> stream = builder.stream("order-events",
            Consumed.with(Serdes.String(), new OrderEventSerde()));

        stream.groupBy((key, order) -> order.getProduct())
              .aggregate(
                  () -> 0.0,
                  (key, order, totalSales) -> totalSales + (order.getPrice() * order.getQuantity()),
                  Materialized.with(Serdes.String(), Serdes.Double()))
              .toStream()
              .to("sales-summary", Produced.with(Serdes.String(), Serdes.Double()));

        return stream;
    }
}
```

âœ… **Kafka Streams continuously updates total sales per product.** ğŸ‰  

---

## **ğŸ“Œ Step 2: Consume Aggregated Data in Reporting Service**
ğŸ“Œ **The reporting service listens for `sales-summary` to generate reports.**

```java
@KafkaListener(topics = "sales-summary", groupId = "reporting-group")
public void consumeSalesData(@Payload String message) {
    System.out.println("Updated sales data: " + message);
}
```

âœ… **Sales insights are now available in real-time.** ğŸ‰  

---

# **ğŸ“Œ Part 3: Detecting Anomalies with Windowing**
## **ğŸ”¹ Why Use Windowing?**
âœ… **Detect fraud by checking multiple transactions in short intervals.**  
âœ… **Identify sudden sales spikes for promotions.**  

### **ğŸ“Œ Implement Window-Based Fraud Detection**
ğŸ“Œ **Detect if a user places orders exceeding â‚¹10,00,000 within 10 minutes.**  

```java
public static class TransactionAggregator {
    private double totalAmount = 0.0;
}

stream.groupBy((key, order) -> order.getOrderId())
      .windowedBy(TimeWindows.of(Duration.ofMinutes(10)))
      .aggregate(
          TransactionAggregator::new,
          (key, order, aggregator) -> {
              aggregator.totalAmount += order.getPrice() * order.getQuantity();
              return aggregator;
          },
          Materialized.with(Serdes.String(), new TransactionAggregatorSerde()))
      .toStream()
      .filter((key, aggregator) -> aggregator.totalAmount > 1000000)
      .to("fraud-alerts", Produced.with(Serdes.String(), new TransactionAggregatorSerde()));
```

âœ… **Fraudulent activity is detected and sent to the `fraud-alerts` topic!** ğŸ‰  

---

# **ğŸ“Œ Part 4: Visualizing Real-Time Analytics with Grafana**
ğŸ“Œ **We will store analytics data in Prometheus and visualize it using Grafana.**  

## **ğŸ”¹ Step 1: Configure Prometheus to Scrape Metrics**
Modify `prometheus.yml`:

```yaml
scrape_configs:
  - job_name: 'kafka-metrics'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['analytics-service:8085']
```

âœ… **Prometheus now scrapes Kafka Streams metrics.**  

---

## **ğŸ”¹ Step 2: Create a Grafana Dashboard**
1ï¸âƒ£ Open **Grafana (`http://localhost:3000`)**  
2ï¸âƒ£ Add **Prometheus as a Data Source**  
3ï¸âƒ£ Create a new **Dashboard**  
4ï¸âƒ£ Add **Real-time Sales Visualization**  
```yaml
query: sum(rate(kafka_sales_summary_total[5m]))
```

âœ… **Real-time sales insights are now displayed in Grafana!** ğŸ‰  

---

# **ğŸ“Œ Final Architecture**
```
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  Orders  ----->  â”‚ Kafka (order-events)     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Sales Aggregation Stream â”‚
                  â”‚ - Computes total sales   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Kafka (sales-summary)    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Reporting Service (Grafana + Prometheus)        â”‚
    â”‚ - Visualizes sales trends                       â”‚
    â”‚ - Detects anomalies                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

âœ… **Real-time analytics is now fully operational!** ğŸ‰  

---

# **ğŸ“Œ Conclusion**
### **ğŸš€ What We Achieved**
âœ… **Implemented Kafka Streams for real-time order processing.**  
âœ… **Computed total sales per product using aggregation.**  
âœ… **Detected anomalies using time-windowed fraud detection.**  
âœ… **Visualized real-time analytics with Grafana & Prometheus.**  

ğŸ”¹ **Next Steps:**  
- Implement **Machine Learning for predictive analytics**.  
- Use **Kafka KSQL for real-time SQL-based querying**.  

> Keep exploring... :)