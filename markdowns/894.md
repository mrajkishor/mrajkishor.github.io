
### **6.1 Matrix Representation of Graphs**

Graphs are widely used in computer science and data structures to model relationships between different entities. A graph consists of:

* **Vertices (or nodes)** – representing entities.
* **Edges** – representing the connections or relationships between those entities.

Each graph can be represented using various matrix-based or list-based models depending on the context, efficiency, and space requirements.

---

### **Types of Graph Representations**

#### **1. Adjacency Matrix**

* An **adjacency matrix** is a square 2D array `V × V`, where `V` is the number of vertices in the graph.
* If there’s an edge between vertex `i` and vertex `j`, then `adj[i][j] = 1`; otherwise, it’s `0`.
* For **undirected graphs**, the matrix is **symmetric**.
* For **weighted graphs**, the matrix stores the **weight** instead of 1s.

**Example:**

* For an undirected graph with vertices A, B, C, D:

  ```
     A B C D
  A  0 1 1 0
  B  1 0 1 0
  C  1 1 0 1
  D  0 0 1 0
  ```

* For a directed graph:

  ```
     A B C D
  A  0 1 1 0
  B  0 0 1 0
  C  0 0 0 1
  D  0 0 0 0
  ```

---

#### **2. Adjacency List**

* This is a space-efficient representation especially useful for **sparse graphs**.
* Each vertex points to a linked list of its adjacent vertices.
* For a graph with vertices 0 to 4, it may look like:

  ```
  0 → 1 → 4
  1 → 0 → 4 → 2 → 3
  2 → 1 → 3
  3 → 1 → 4 → 2
  4 → 3 → 0 → 1
  ```

**Advantages:**

* Saves space for large sparse graphs.
* Faster edge insertions and deletions.

**Disadvantages:**

* Slower when checking if an edge exists between two specific vertices (compared to matrix).

---

#### **3. Incidence Matrix**

* An incidence matrix is a 2D matrix where **rows represent vertices** and **columns represent edges**.
* It is used mostly for **directed graphs**.
* Values:

  * `1` → Vertex has an outgoing edge to that column.
  * `-1` → Vertex has an incoming edge from that column.
  * `0` → No connection with that edge.

**Example:**

```
Vertices → Rows
Edges    → Columns

     E1 E2 E3 E4 ...
A →  1  1 -1  0 ...
B → -1  0  0  1 ...
C →  0 -1  0  1 ...
```

This form is more compact for algorithms needing both vertex and edge information directly.

---

These matrix forms help perform various operations like path-finding, connectivity checks, and graph traversals using linear algebra and matrix manipulation techniques.

### **6.2 Storage Representation and Manipulation of Graphs**

A graph is a way to represent relationships between different things—like people in a social network or cities connected by roads. It mainly has two parts:

* **Vertices (or Nodes)** – These are the points, like the people or cities.
* **Edges** – These are the connections between those points, like friendships or roads.

To work with graphs using a computer, we need to store them in a form that’s easy to process. There are two popular ways to store graphs in memory:

---

### **1. Adjacency List**

* Think of this like a list where each node keeps track of who it's directly connected to.
* Each **node** is stored as an index in a 1D array.
* For each node, we maintain a **linked list** (or array) of all the nodes it's connected to.

**Example**:
If Node 0 is connected to Nodes 1 and 3, its list will be: `0 → 1 → 3`

**Advantages:**

* Saves space, especially for sparse graphs (graphs with fewer edges).
* Easy to add or remove connections.

**Disadvantages:**

* Checking if a specific edge exists can take longer because you may need to scan the list.

---

### **2. Adjacency Matrix**

* Here, we use a **2D matrix** (table) where both rows and columns represent nodes.
* Each cell `(i, j)` is set to `1` if there is an edge from node `i` to node `j`; otherwise, it’s `0`.

**Example**:
If Node 2 is connected to Node 5, then cell `[2][5] = 1`.

**Advantages:**

* Quick to check if an edge exists between two nodes.
* Great for dense graphs (graphs with lots of connections).

**Disadvantages:**

* Takes up more memory, especially if the graph has very few edges.

---

### **Transpose of a Graph**

The **transpose** of a directed graph is just the same graph with all the arrows (edges) reversed.

So, if there’s an edge from `A → B` in the original graph, in the transposed version, it becomes `B → A`.

**Why it’s useful?**

* Used in algorithms like **Kosaraju’s** to find strongly connected components.

**How to compute it:**

* If you're using an **adjacency list**, for every edge from `u → v`, just add an edge from `v → u` in a new graph.
* If you're using an **adjacency matrix**, simply **transpose the matrix** (flip it along its diagonal).

**Time Complexity:** `O(V + E)` — because we touch each vertex and each edge once.

---

### **Summary Table**

| Representation   | Space Efficiency | Best For                     | Notes             |
| ---------------- | ---------------- | ---------------------------- | ----------------- |
| Adjacency List   | Very efficient   | Sparse Graphs                | Easy to modify    |
| Adjacency Matrix | Uses more space  | Dense Graphs                 | Fast edge lookup  |
| Transpose        | -                | Used in DFS/Graph Algorithms | Reverse all edges |

---


### 6.3 Euler Graphs

An **Euler graph** is a special kind of connected graph where you can draw a path that uses every edge exactly once **and** ends up right where you started. This kind of path is called an **Euler circuit**. If the path uses every edge just once but starts and ends at different points, it's called an **Euler path**.

#### 🔁 Euler Circuit (or Euler Tour)

* A **circuit** means the path starts and ends at the same vertex.
* Every edge is visited exactly once.
* For a graph to have an Euler circuit, **all vertices must have even degrees** (i.e., even number of edges touching them).
* The graph must also be **connected** (you can get from any node to any other node).

#### ➡️ Euler Path (but not circuit)

* A graph has an Euler path **but not a circuit** if:

  * It is connected, and
  * **Exactly two vertices have odd degrees**.
* The path will start and end at the two odd-degree vertices.

---

### ✅ Example of Euler Graph

Take a look at the first graph you shared (with vertices `a → b → c → d → e → f → a`):

* Every vertex has an **even degree** (2 or 4).
* There's a closed trail that visits all edges **once**, and it ends where it began.
* Therefore, this is an **Euler circuit** and the graph is an **Euler graph**.

---

### ❌ Example of a Non-Euler Graph

Now check the second graph:

* Vertices `b` and `d` have a degree of 3 (which is **odd**).
* Since all vertices don't have even degrees, it **can't** have an Euler circuit.
* It also doesn’t qualify for an Euler path either, because **more than two** vertices have odd degrees.
* Hence, this graph is **not an Euler graph**.

---

### Summary:

| Property             | Euler Circuit | Euler Path             |
| -------------------- | ------------- | ---------------------- |
| Must use all edges?  | ✅ Yes         | ✅ Yes                  |
| Can repeat vertices? | ✅ Yes         | ✅ Yes                  |
| Repeats edges?       | ❌ No          | ❌ No                   |
| Starts = Ends?       | ✅ Yes         | ❌ No                   |
| All vertices even?   | ✅ Required    | ❌ Only two odd allowed |

---



### 🔁 **6.4 Hamiltonian Path and Circuits**

A **Hamiltonian Path** is a path in a graph that visits **every vertex exactly once**, without repeating any. If this path **starts and ends at the same vertex**, it becomes a **Hamiltonian Circuit** (also called a Hamiltonian Cycle).

#### ✅ Basic Characteristics:

* You must cover **each vertex once**.
* **Edges may or may not be used** more than once (though in practice, usually they aren't).
* **No rule** guarantees that a graph has a Hamiltonian Path or Circuit—this makes finding them harder than Euler circuits.

---

### 🧠 **Key Definitions:**

* **Hamiltonian Path**: Visits every vertex **exactly once**, but doesn't need to end where it started.
* **Hamiltonian Circuit**: A closed loop that visits every vertex **once** and **returns** to the starting vertex.

---

### 📘 Example:

From the first diagram, the graph contains the circuit:

```
A → B → C → D → E → F → A
```

This is a **Hamiltonian Circuit** since it:

* Covers every vertex once.
* Returns to the starting vertex A.

From the second diagram, the path:

```
E → A → B → C → D
```

is a **Hamiltonian Path**, because:

* It visits every vertex (A, B, C, D, E) only once.
* It doesn’t loop back to E.

---

### 📏 **Theorems Related to Hamiltonian Graphs:**

1. **Dirac’s Theorem**
   If a graph has **n ≥ 3 vertices** and **every vertex has degree ≥ n/2**, then the graph **must** have a Hamiltonian Circuit.

2. **Ore’s Theorem**
   If for every **pair of non-adjacent vertices (u, v)**, the sum of their degrees is **at least n**, then the graph has a Hamiltonian Circuit.

---

### 🚫 Difference from Euler Path:

* **Hamiltonian Path** focuses on **visiting every vertex once**.
* **Euler Path** focuses on **traversing every edge once**.

---


### **6.5 Graph Traversals**

When we talk about graph traversal, we're referring to the process of visiting each vertex in a graph—typically with the goal of examining or processing its structure. The order in which we visit these nodes defines the type of traversal. Unlike trees, graphs may have cycles, so special care must be taken to avoid revisiting nodes.

There are two major graph traversal methods:

---

### **6.5.1 Breadth-First Search (BFS)**

Breadth-First Search works level by level. Starting from a source node, it visits all the nodes one "layer" away, then the next layer, and so on. It uses a **queue** to keep track of the nodes that need to be explored.

* **Steps:**

  1. Start at the root node.
  2. Mark it visited and add it to a queue.
  3. Explore all its neighbors.
  4. Repeat the process for each neighbor.

* **Example Orders:**
  From the graph shown:

  * BFS Traversals could be: `2 3 0 1` or `2 0 3 1` (depending on the order neighbors are added)

* **Applications:**

  * Detecting cycles
  * Finding the shortest path in an unweighted graph
  * Web crawlers
  * GPS navigation systems
  * Peer-to-peer networks like BitTorrent
  * Finding people within K levels in a social network
  * Broadcasting in networks

---

### **6.5.2 Depth-First Search (DFS)**

Depth-First Search dives as deep as possible down one path before backtracking. It uses a **stack** (either explicitly or via recursion).

* **Steps:**

  1. Start at the root node.
  2. Visit it, mark it visited, and explore one of its neighbors.
  3. Continue this process until no unvisited neighbors are left.
  4. Backtrack and repeat for other neighbors.

* **Example Traversal:**
  DFS Traversal might be: `A D F C E B`

* **Applications:**

  * Finding strongly connected components
  * Solving puzzles or mazes (like backtracking)
  * Generating mazes
  * Topological sorting
  * Model checking (verifying system properties)
  * Detecting cycles
  * Solving AI search problems

---

### **Comparison (From Diagram)**

| Feature        | BFS                            | DFS                          |
| -------------- | ------------------------------ | ---------------------------- |
| Data Structure | Queue                          | Stack                        |
| Search Style   | Level-by-level                 | Deep dive until dead-end     |
| Use Case       | Shortest path, social networks | Puzzles, backtracking        |
| Space Usage    | Can be higher (wide graphs)    | Lower for narrow/deep graphs |

---

### **Important Note**

Both BFS and DFS can handle disconnected graphs and graphs with cycles by maintaining a **visited list** to avoid reprocessing the same nodes.



---

### **6.6 Shortest Path in Weighted Graphs (Dijkstra’s Algorithm)**

When you're trying to find the most efficient way to get from one point to another in a network—say, the shortest driving route between cities or the quickest data route in a network—you’re solving a shortest path problem. For weighted graphs, the best-known algorithm for this is **Dijkstra’s Algorithm**.

---

### **What It Does:**

Dijkstra’s Algorithm helps you find the shortest (least cost or distance) path between two points in a **weighted graph**, where edges have weights (or costs). It always picks the path with the **lowest total cost**, even if it means taking more steps (nodes) than another option.

---

### **Real Life Example:**

Think of Google Maps. When you want to travel from your current location to your destination, it doesn’t always choose the path with the fewest turns. Instead, it picks the path with the **least travel time**, even if it goes around a bit more. That’s exactly what Dijkstra’s algorithm does.

---

### **Working Principle (Step-by-Step):**

Here’s how the algorithm works in simple terms:

1. **Start from the Source Node (vs):**
   Assign it a cost of 0 (`P(vs) = 0`). For all other nodes, the cost is ∞ (infinity), meaning they're unreachable for now.

2. **Visit All Nodes (Loop):**
   Repeat the process for all nodes (from `i = 0` to `|V|-1`, where `V` is the number of vertices).

3. **Relaxation Step (Main Logic):**
   For each node, check its neighbors:

   * If going through the current node offers a shorter path to the neighbor, **update the neighbor’s cost**.
   * Mathematically:
     `T(vi) = min [T(vi), P(vj) + lij]`
     (meaning: is the new cost via `vj` better than the current one?)

4. **Pick the Node with Minimum Tentative Cost:**
   This becomes the next node to process. Save this update in your cost table `P`.

5. **Repeat Until All Nodes Are Processed.**

6. **End:**
   Once you finish visiting all nodes (i.e., `i = V - 1`), the algorithm ends. Your table now has the shortest distances from the source node to every other node.

---

### **Formula Summary:**

If at any point:

```
dist(u) + len(u, v) < dist(v)
```

then:

```
dist(v) = dist(u) + len(u, v)
```

This just means: "If going through node `u` gives a better (shorter) path to node `v`, then update the distance to `v`."

---

### **Visual Flow (From Diagram):**

* **Start** with source node
* **Initialize distances** (source = 0, others = ∞)
* **Iterate through all nodes**
* **Update (relax) neighbors' distances**
* **Pick the next closest node**
* **Repeat until done**

---

### **Why It’s Called “Greedy”?**

Dijkstra’s algorithm is considered greedy because, at every step, it picks the **locally best (shortest) path**, hoping it leads to the globally best solution. And it works!

---


### **6.7 Graph Isomorphism and Homomorphism**

---

### **6.7.1 Graph Isomorphism**

Two graphs are said to be **isomorphic** if they are essentially the same in structure, even if they look different visually.

* Think of it like this: If you can **relabel** the nodes of one graph and make it look **identical** to another, then those graphs are **isomorphic**.

#### ✅ **Conditions for Isomorphism:**

* Same number of **vertices**
* Same number of **edges**
* Same **degree sequence** (i.e., the number of edges attached to each vertex is the same across graphs)
* Same **connected structure** (adjacency relations)

#### ❌ **Conditions that Disprove Isomorphism:**

If any of these don't match, the graphs are **not** isomorphic:

1. Different number of **connected components**
2. Different number of **vertices**
3. Different number of **edges**
4. Different **vertex degree sequences**

#### 📌 **Example:**

In the image you shared:

* All three diagrams on the left show **isomorphic graphs**—they’re just drawn differently.
* The one on the right shows **non-isomorphic structure**, even though the nodes are labeled similarly—it doesn’t preserve edge relationships.

---

### **6.7.2 Graph Homomorphism**

A **homomorphism** is a bit more relaxed than isomorphism.

* It’s a **mapping from one graph to another** that **preserves adjacency**—meaning, if two nodes are connected in graph G, their mapped counterparts must also be connected in graph H.
* However, the mapping **doesn't need to be bijective** (i.e., one-to-one and onto).

#### 🧠 **Real-life analogy:**

If graph G is a complex network, and graph H is a simpler version of it, a homomorphism would compress G into H **without breaking connections**.

---

### **Properties of Homomorphisms:**

1. If the mapping is **bijective** and **structure-preserving**, it becomes an **isomorphism**.
2. Homomorphisms **preserve edges and connectivity**—edges in G must map to edges in H.
3. **Composing** two homomorphisms gives you another valid homomorphism.
4. Checking for graph homomorphism is **NP-complete**—it’s computationally hard to find or verify.

---

### ✅ Summary Table

| Concept         | Isomorphism                      | Homomorphism                            |
| --------------- | -------------------------------- | --------------------------------------- |
| Type of mapping | Bijective (1-to-1, onto)         | Not necessarily bijective               |
| Preserves       | Structure, edges, degrees        | Just adjacency                          |
| Visual check    | Can be redrawn to look identical | May look different but keeps edge logic |
| Difficulty      | Easier to disprove               | NP-complete (harder to prove)           |


