

## üß† **Cache Memory ‚Äì Detailed Note**

### ‚úÖ **What is Cache Memory?**

**Cache Memory** is a **very high-speed memory** placed between the **CPU and Main Memory (RAM)**. It acts as a **buffer** to temporarily store frequently accessed instructions and data, allowing the CPU to retrieve them **quickly**, thereby improving overall system performance.

* **Faster than main memory**, but **slower than CPU registers**
* **More expensive** than RAM but **cheaper** than registers
* **Volatile** memory; data is lost when power is off

---

### ‚öôÔ∏è **Purpose of Cache Memory**

* Reduces **latency** in accessing frequently used data
* Minimizes **CPU waiting time**
* Acts as a **local copy** of active memory blocks
* Improves **instruction execution speed**

---

### üî¢ **Hierarchy Levels of Memory**

Cache memory is part of a broader **memory hierarchy**:

| Level | Memory Type       | Speed     | Size      | Volatility   |
| ----- | ----------------- | --------- | --------- | ------------ |
| L0    | CPU Registers     | Fastest   | Few Bytes | Volatile     |
| L1    | Cache Memory      | Very Fast | KB-MB     | Volatile     |
| L2    | Main Memory (RAM) | Fast      | GB        | Volatile     |
| L3    | Secondary Storage | Slow      | TB        | Non-Volatile |
| L4    | Tertiary Storage  | Very Slow | TB+       | Non-Volatile |

---

### üìà **Cache Performance Terms**

* **Cache Hit**: The data requested by the CPU is **found in cache**
* **Cache Miss**: The data is **not in cache**; it must be fetched from main memory
* **Hit Ratio** = `Hits / (Hits + Misses)` ‚Üí higher is better

---

### üß≠ **Cache Mapping Techniques**

These define **how data from main memory is mapped into cache**.

#### 1. üîπ **Direct Mapping**

* Each memory block maps to **only one cache line**
* Uses: `Cache Index = (Block Address) % (Number of Cache Lines)`
* Simple and fast, but **can cause conflicts** when multiple blocks map to the same line

**Pros**: Easy to implement
**Cons**: High conflict misses

---

#### 2. üî∏ **Set-Associative Mapping**

* Cache is divided into **sets**, and each set has multiple lines
* A block maps to a specific set but can occupy **any line** within the set

**Working:**

1. Use bits of the memory address to **select a set**
2. All lines in the set are **checked for a tag match**
3. If found ‚Üí cache hit, else cache miss

**Types**:

* 2-way, 4-way, 8-way... based on number of lines per set

**Pros**: Balanced trade-off between speed and flexibility
**Cons**: More complex than direct mapping

---

#### 3. üü¢ **Fully Associative Mapping**

* A memory block can be stored **in any line** of the cache
* **Complete flexibility**, but requires **comparisons with all cache lines**

**Working:**

* Each cache line stores both the **data** and its **full memory address (tag)**
* Associative logic checks all tags simultaneously

**Pros**: Minimal conflict misses
**Cons**: Expensive hardware, slower comparisons

---

### üßÆ **Example Comparison**

| Technique               | Mapping Flexibility | Hit Rate Potential | Hardware Cost |
| ----------------------- | ------------------- | ------------------ | ------------- |
| Direct Mapping          | Low                 | Moderate           | Low           |
| Set-Associative Mapping | Medium              | High               | Moderate      |
| Fully Associative       | High                | Highest            | High          |

---

### üß† **Modern CPU Cache Levels**

* **L1 Cache**: Smallest, fastest, located inside CPU core
* **L2 Cache**: Bigger, slower than L1, shared across cores or per-core
* **L3 Cache**: Shared by all cores, bigger and slower than L2

---

### üßµ **Conclusion**

Cache memory is **critical** for modern computer architecture. By storing frequently accessed data near the CPU, it **significantly reduces data access time**, thereby **boosting system performance**. The cache hierarchy and mapping techniques help balance **cost, speed, and complexity**.

