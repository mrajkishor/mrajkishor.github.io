

# üß† Decision Tree Induction, Bayes Classification, and Rule-Based Classification

---

## üìå 1. **Introduction to Classification Techniques**

Classification is a **supervised learning** technique used to assign predefined labels to input data. It is widely applied in:

* Predicting customer behavior
* Disease diagnosis
* Spam detection
* Fraud detection

Two commonly used methods:

* **Decision Tree Induction**
* **Bayes Classification**

---

## üå≥ 2. **Decision Tree Induction**

### üîπ Definition:

![alt text](dm_decision_tree.jpg)

A decision tree is a **hierarchical model** in the form of a tree used to predict the value of a target variable based on input features. It consists of:

* **Root Node**: Top-most decision attribute
* **Internal Nodes**: Test conditions on attributes
* **Branches**: Outcomes of the tests
* **Leaf Nodes**: Final classification label

---

### üîπ How Decision Trees Work:

A decision tree recursively splits the dataset using the most **discriminative feature**.

#### üßÆ Splitting Criteria:

* **Information Gain (Entropy-based)**:

  $$
  IG(D, A) = Entropy(D) - \sum \frac{|D_i|}{|D|} \cdot Entropy(D_i)
  $$

  Used by **ID3** algorithm. Prefers attributes that produce **pure subsets**.

* **Gini Index**:

  $$
  Gini(D) = 1 - \sum_{i=1}^{c} p_i^2
  $$

  Used by **CART**. Measures impurity; lower is better.

---

### üîπ Stopping Criteria:

* Max depth reached
* Minimum samples in node
* No further gain in splitting

---

### üîπ Example: Banking Loan Default Prediction

**Attributes**: Credit Score, Income, Employment

**Root Node**: Credit Score

**Branches**: High vs Low

**Leaves**: ‚ÄúDefault‚Äù / ‚ÄúNo Default‚Äù

![alt text](image-62.png)

---

### ‚úÖ Advantages:

* Highly **interpretable**
* Handles **categorical and numerical** data
* No need for normalization

### ‚ùå Disadvantages:

* **Overfitting** if not pruned
* Unstable to small changes in data
* Can grow **very large**

---

## üìä 3. **Bayes Classification**

### üîπ Definition:

Uses **Bayes‚Äô Theorem** to predict the probability that a data point belongs to a particular class.

$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$

Where:

* $P(A|B)$: Posterior probability
* $P(B|A)$: Likelihood
* $P(A)$: Prior
* $P(B)$: Evidence

---

### üîπ Naive Bayes Classifier

Assumes that all features are **independent**.

#### Example: Spam Classification

* Features: Words like ‚Äúwin‚Äù, ‚Äúfree‚Äù
* Naive Bayes estimates:

  $$
  P(\text{spam}|\text{words}) \propto P(\text{words}|\text{spam}) \cdot P(\text{spam})
  $$

---

### ‚úÖ Advantages:

* Very **fast and scalable**
* Requires **less training data**
* Works well with **text data**

### ‚ùå Disadvantages:

* **Strong independence assumption** rarely holds
* Performs poorly with **correlated** features

---

## üìú 4. **Rule-Based Classification**

### üîπ Definition:

Uses human-readable **if‚Äìthen rules** to classify data.

#### Example:

```
IF age > 30 AND income > 50,000 THEN buy_car = yes
```

---

### üîπ Rule Extraction:

* Derived from decision trees or association rule mining
* Expressed as:

  ```
  IF (condition) THEN (class)
  ```

### üîπ Rule Pruning:

Removes redundant or overfitted rules using a **validation set** to improve generalization.

---

### ‚úÖ Applications:

* **Expert Systems** (e.g., medical diagnosis)
* **Credit Scoring** (loan approvals)
* **Customer Segmentation** (e-commerce targeting)

#### Indian Context Example:

```
IF purchases include electronics AND income > ‚Çπ10L THEN recommend premium electronics
```

---

### ‚úÖ Advantages:

* **Highly interpretable**
* Easily **customizable**
* Embeds **domain knowledge**

### ‚ùå Disadvantages:

* Not scalable to **very large** datasets
* Can become **complex** with many rules
* **Computational cost** of rule mining is high

---

## üìö Summary Table

| Method        | Key Idea                     | Pros                            | Cons                         |
| ------------- | ---------------------------- | ------------------------------- | ---------------------------- |
| Decision Tree | Tree of decisions            | Interpretable, no preprocessing | Overfitting, instability     |
| Naive Bayes   | Bayes Theorem + independence | Fast, good for text             | Assumes feature independence |
| Rule-Based    | If-Then rules                | Transparent, editable           | Not scalable, rule explosion |

