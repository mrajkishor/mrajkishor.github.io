
# üöÄ Mastering Dynamic Programming (DP): Use Cases & Patterns

Dynamic Programming (DP) is one of the most powerful algorithmic techniques in problem-solving interviews and competitive programming. If you‚Äôve ever solved a problem and thought, ‚ÄúI‚Äôm recalculating the same thing again and again,‚Äù then **DP is your friend**.

---

## üéØ What Is Dynamic Programming?

Dynamic Programming is an optimization technique used to **avoid redundant calculations** by storing results of subproblems and reusing them when needed.

It applies when:
- **The problem has overlapping subproblems**
- **The problem has optimal substructure** (solution can be built from solutions of subproblems)

---

## üß© Two Core Techniques in DP

### 1Ô∏è‚É£ Top-Down Approach (Memoization)
- Solve the main problem by **recursively solving subproblems**
- Store answers in a **map or array** to avoid recomputation

```java
// Java Example: Fibonacci with memoization
Map<Integer, Integer> memo = new HashMap<>();

int fib(int n) {
    if (n <= 1) return n;
    if (memo.containsKey(n)) return memo.get(n);
    int result = fib(n - 1) + fib(n - 2);
    memo.put(n, result);
    return result;
}
```

---

### 2Ô∏è‚É£ Bottom-Up Approach (Tabulation)
- **Start from the base case** and iteratively build up the solution
- Avoids recursion (better space and performance)

```java
// Java Example: Fibonacci with tabulation
int fib(int n) {
    if (n <= 1) return n;
    int[] dp = new int[n + 1];
    dp[0] = 0; dp[1] = 1;
    for (int i = 2; i <= n; i++)
        dp[i] = dp[i - 1] + dp[i - 2];
    return dp[n];
}
```

---

## üí° When to Use DP ‚Äì Common Use Cases

### üß† Optimal Choices / Overlapping Subproblems
Problems where you must make a choice at each step and future outcomes depend on that decision.  
**Examples:** Fibonacci, Climbing Stairs, Min Cost Climbing

---

### üéí 1. Knapsack / Subset Problems
These involve choices with weights or values:
- **0/1 Knapsack** ‚Äì Choose items with or without including them
- **Subset Sum** ‚Äì Is there a subset with a given sum?

üëâ Use a **2D or 1D DP array** based on space optimization.

---

### ‚úÇÔ∏è 2. Edit Distance / Longest Common Subsequence (LCS)
Used in string transformation problems:
- **Edit Distance** ‚Äì Min operations to convert one string to another
- **LCS** ‚Äì Longest subsequence common to both strings

üëâ Use **DP Matrix** (`dp[i][j]`) where `i` and `j` are string indices.

---

### üí∞ 3. Coin Change Variants
- **Minimum coins to make amount**
- **Count ways to make amount**

üëâ Recurrence builds upon combinations of coin denominations.

---

### üì¶ 4. Partition Problems
- Equal Partition of array
- Maximize the minimum difference

üëâ Reduces to **subset sum variations**.

---

### ‚õèÔ∏è 5. Cutting Problems
- **Rod Cutting** ‚Äì Max revenue from rod pieces
- **Palindrome Partitioning** ‚Äì Min cuts to make all substrings palindromic

üëâ Requires splitting at multiple points and checking combinations.

---

### üìà 6. Maximum Subarray Sum ‚Äì Kadane‚Äôs Algorithm
- Tracks `localMax` and updates `globalMax`
- Classic problem: [Leetcode #53](https://leetcode.com/problems/maximum-subarray/)

```java
int maxSubArray(int[] nums) {
    int currentMax = nums[0], globalMax = nums[0];
    for (int i = 1; i < nums.length; i++) {
        currentMax = Math.max(nums[i], currentMax + nums[i]);
        globalMax = Math.max(globalMax, currentMax);
    }
    return globalMax;
}
```

---

## üîç How to Recognize a DP Problem

Ask:
- Can I divide the problem into **smaller overlapping subproblems**?
- Does the final answer **depend on previous states**?
- Are there **too many recursive calls** (causing TLE)?  
If yes, it‚Äôs time to **switch to DP**.

---

## ‚úÖ Final Tip: Build a Template
1. Define state (`dp[i]` or `dp[i][j]`)
2. Define base cases
3. Write recurrence
4. Implement in top-down or bottom-up
5. Optimize space if needed

---

## üß™ Practice Problems
| Problem | Type |
|--------|------|
| Climbing Stairs | Fibonacci-style |
| House Robber | Linear DP |
| Longest Palindromic Substring | 2D DP |
| Subset Sum | Knapsack |
| Edit Distance | DP Matrix |

---

#### Q. Why memoisation is not used in tabulation?

Here's the clear reasoning:

---

### üîÅ **Memoization vs. Tabulation**

| Aspect | Memoization (Top-Down) | Tabulation (Bottom-Up) |
|--------|------------------------|-------------------------|
| Style | Recursive | Iterative |
| How It Works | Calls subproblems on-demand, caches results | Precomputes all subproblems |
| Cache Use | Only when needed | Fills whole table regardless |
| Flow | From main problem ‚Üí base case | From base case ‚Üí main problem |

---

### üí° **Why Memoization is *not* used in Tabulation**

Because **tabulation computes everything in order from the bottom**, there is **no need to check** whether a value was already computed (like memoization does). You're controlling the flow step-by-step.

> In memoization, you ask: "Did I solve this subproblem before?"  
> In tabulation, you say: "Let me solve **every** subproblem in order."

---

### üîç Analogy

Memoization = Lazy evaluation (only when required)  
Tabulation = Eager evaluation (compute all up front)

---

So, memoization is not used in tabulation because:
- Tabulation ensures all required subproblems are already computed when needed
- There‚Äôs no recursive call stack to trigger lazy lookup
- It‚Äôs redundant and inefficient to check for previously filled values in a loop

---

#### Q. Which is better approach (memoisation or tabulation) ?


Here‚Äôs a quick comparison to help decide which approach is better:

---

### üßÆ **Memoization (Top-Down)**
- ‚úÖ Good when **only some subproblems** are needed
- ‚úÖ Easier to write recursively
- ‚ùå Can cause **stack overflow** if recursion depth is high
- ‚úÖ Great for problems with **sparse overlapping subproblems**

---

### üß± **Tabulation (Bottom-Up)**
- ‚úÖ Avoids recursion ‚Üí **no stack overflow**
- ‚úÖ Usually **faster** in practice due to tight loops
- ‚úÖ Ideal when **all subproblems** are needed
- ‚úÖ **Easier to optimize space**

---

### üîç Verdict

| Use Case                              | Better Approach       |
|---------------------------------------|------------------------|
| Deep recursion or space optimization  | **Tabulation**         |
| Partial computation or branching      | **Memoization**        |
| Large input size                      | **Tabulation**         |
| Learning or debugging                 | **Memoization** (easier to trace) |

---

**üëâ In interviews**: Start with memoization for clarity, switch to tabulation if performance or space becomes critical.

---

==A List of **Dynamic Programming problems** that specifically involve **optimal choices and overlapping subproblems** ‚Äî perfect for mastering both memoization and tabulation:==



### üéØ **Optimal Choices / Overlapping Subproblems ‚Äì Problem List**

| # | Problem Name | Platform | Pattern |
|---|--------------|----------|---------|
| 1Ô∏è‚É£ | [Climbing Stairs](https://leetcode.com/problems/climbing-stairs/) | LeetCode | Fibonacci-style DP |
| 2Ô∏è‚É£ | [House Robber](https://leetcode.com/problems/house-robber/) | LeetCode | Choose/Skip Pattern |
| 3Ô∏è‚É£ | [House Robber II](https://leetcode.com/problems/house-robber-ii/) | LeetCode | Circular DP |
| 4Ô∏è‚É£ | [Paint House](https://leetcode.com/problems/paint-house/) | LeetCode | Min Cost |
| 5Ô∏è‚É£ | [Partition Equal Subset Sum](https://leetcode.com/problems/partition-equal-subset-sum/) | LeetCode | Subset Sum DP |
| 6Ô∏è‚É£ | [Coin Change](https://leetcode.com/problems/coin-change/) | LeetCode | Minimize Coins |
| 7Ô∏è‚É£ | [Coin Change II](https://leetcode.com/problems/coin-change-ii/) | LeetCode | Count Ways |
| 8Ô∏è‚É£ | [0/1 Knapsack Problem](https://practice.geeksforgeeks.org/problems/0-1-knapsack-problem0945/1) | GFG | Classic Choice DP |
| 9Ô∏è‚É£ | [Unbounded Knapsack](https://www.geeksforgeeks.org/unbounded-knapsack-repetition-items-allowed/) | GFG | Reuse Items |
| üîü | [Minimum Path Sum](https://leetcode.com/problems/minimum-path-sum/) | LeetCode | Grid Path DP |

---

### üìö String-Based Optimal Choice DP

| # | Problem Name | Platform | Pattern |
|---|--------------|----------|---------|
| 1Ô∏è‚É£1Ô∏è‚É£ | [Longest Common Subsequence](https://leetcode.com/problems/longest-common-subsequence/) | LeetCode | 2D DP |
| 1Ô∏è‚É£2Ô∏è‚É£ | [Longest Palindromic Subsequence](https://leetcode.com/problems/longest-palindromic-subsequence/) | LeetCode | Choice between ends |
| 1Ô∏è‚É£3Ô∏è‚É£ | [Edit Distance](https://leetcode.com/problems/edit-distance/) | LeetCode | Insert/Delete/Replace |
| 1Ô∏è‚É£4Ô∏è‚É£ | [Palindrome Partitioning II](https://leetcode.com/problems/palindrome-partitioning-ii/) | LeetCode | Min Cut |
| 1Ô∏è‚É£5Ô∏è‚É£ | [Distinct Subsequences](https://leetcode.com/problems/distinct-subsequences/) | LeetCode | Include/Exclude |
| 1Ô∏è‚É£6Ô∏è‚É£ | [Interleaving String](https://leetcode.com/problems/interleaving-string/) | LeetCode | Merge 2 strings |

---

### ‚öôÔ∏è Matrix / Game / State Transition DP

| # | Problem Name | Platform | Pattern |
|---|--------------|----------|---------|
| 1Ô∏è‚É£7Ô∏è‚É£ | [Burst Balloons](https://leetcode.com/problems/burst-balloons/) | LeetCode | Interval DP |
| 1Ô∏è‚É£8Ô∏è‚É£ | [Matrix Chain Multiplication](https://www.geeksforgeeks.org/matrix-chain-multiplication-dp-8/) | GFG | Parenthesization |
| 1Ô∏è‚É£9Ô∏è‚É£ | [Stone Game](https://leetcode.com/problems/stone-game/) | LeetCode | MinMax DP |
| 2Ô∏è‚É£0Ô∏è‚É£ | [Minimum Cost Tree From Leaf Values](https://leetcode.com/problems/minimum-cost-tree-from-leaf-values/) | LeetCode | Divide & Conquer |

---

**Note:**

‚úÖ **Tabulation usually takes more space than recursion** ‚Äî *unless* you optimize it.


### üìä Tabulation (Bottom-Up)
- Uses an **array or matrix** to store *all* subproblem results.
- So it takes **O(n)** or **O(n√óm)** space depending on the problem.

---

### üîÅ Recursion with Memoization (Top-Down)
- Uses a **cache + call stack**:
  - Cache: stores results ‚Üí O(n)
  - Call stack: adds extra space ‚Üí O(n) worst case

---

### üîÑ Pure Recursion (No Memoization)
- Uses **only stack space**, but leads to **exponential time** and possibly stack overflow.

---

### üß† Optimization Tip
Many tabulation problems can be **space optimized**:
- **Fibonacci:** from `O(n)` ‚Üí `O(1)`
- **2D DP problems (e.g. LCS):** from `O(n√óm)` ‚Üí `O(min(n, m))`

---

### ‚úÖ Final Verdict

| Approach         | Space Complexity |
|------------------|------------------|
| Tabulation       | ‚úÖ Predictable but higher (O(n), O(n√óm)) |
| Memoization      | ‚ö†Ô∏è Depends on recursion depth + cache |
| Space-Optimized Tabulation | ‚úÖ Best of both (O(1), O(n)) |

---

**Tabulation may use more space**, but it's **safer and more stack-friendly** than deep recursion.

### üîß How Tabulation Space is Optimized

Instead of storing **all subproblems**, we observe that:
> Most DP problems only need the **last few states**, not the entire table.

So we can **reduce space** by storing only what's necessary.

---

### ‚úÖ Common Techniques

#### 1Ô∏è‚É£ **Rolling Array (1D Optimization)**
Use two variables or a small array instead of the full `dp[]`.

**Example: Fibonacci**
```java
int fib(int n) {
    if (n <= 1) return n;
    int prev2 = 0, prev1 = 1, curr = 0;
    for (int i = 2; i <= n; i++) {
        curr = prev1 + prev2;
        prev2 = prev1;
        prev1 = curr;
    }
    return curr;
}
```
üß† From `O(n)` ‚Üí `O(1)` space

---

#### 2Ô∏è‚É£ **Row Compression (2D ‚Üí 1D)**
Use one or two rows in problems like LCS, Edit Distance, Knapsack.

**Example: LCS**
```java
int lcs(String a, String b) {
    int[] prev = new int[b.length() + 1];
    int[] curr = new int[b.length() + 1];

    for (int i = 1; i <= a.length(); i++) {
        for (int j = 1; j <= b.length(); j++) {
            curr[j] = a.charAt(i-1) == b.charAt(j-1) 
                      ? 1 + prev[j-1] 
                      : Math.max(prev[j], curr[j-1]);
        }
        int[] temp = prev;
        prev = curr;
        curr = temp;
    }
    return prev[b.length()];
}
```
üß† From `O(n√óm)` ‚Üí `O(m)`

---

#### 3Ô∏è‚É£ **In-Place Updates**
When updates don‚Äôt interfere, reuse the same array for both reads and writes.

---

### üìå Summary Table

| Optimization | Use Case | From | To |
|--------------|----------|------|----|
| Rolling Variables | Linear DP (Fibonacci, Staircase) | O(n) | O(1) |
| Row Compression | 2D DP (LCS, Knapsack) | O(n√óm) | O(min(n,m)) |
| In-Place DP | Safe overwrite (like Prefix Sum problems) | O(n) | O(1) |


---


## Q. Does memoisation uses more stack spaces while tabulation uses more heap space?



### üß† **Memoization**
- üìå **Top-down recursive**
- üóÇÔ∏è Uses **call stack** ‚Üí occupies **stack memory**
- üßæ Stores results in a cache (map/array) ‚Üí uses **heap memory**
- ‚ùå Deep recursion = **stack overflow risk**

---

### üìä **Tabulation**
- üìå **Bottom-up iterative**
- üîÅ No recursion ‚Üí minimal **stack memory**
- üßæ Uses a full DP table in **heap memory**
- ‚úÖ Safer for large input sizes

---

### ‚öñÔ∏è Summary Table

| Approach     | Stack Memory        | Heap Memory        | Risk |
|--------------|---------------------|---------------------|------|
| Memoization  | üîº High (recursive)  | ‚úÖ Medium (cache)   | ‚ö†Ô∏è Stack Overflow |
| Tabulation   | ‚úÖ Low (iterative)   | üîº Higher (DP table) | ‚úÖ Safe |

---

üí° So yes:
> **Memoization = more stack memory**  
> **Tabulation = more heap memory**

---

### Q. Why stack memory is easily overflowed but not heap?

Great question! Here's the intuitive and technical answer:

---

### üß† **Stack Memory vs Heap Memory ‚Äî Why Stack Overflows Easily**

| Feature            | **Stack**                             | **Heap**                            |
|--------------------|----------------------------------------|-------------------------------------|
| üíæ Size Limit      | **Very small** (typically 1‚Äì2 MB per thread) | **Very large** (can be GBs)         |
| üåÄ Purpose          | Stores **function call frames**        | Stores **objects and data**         |
| ‚è±Ô∏è Allocation Speed | **Very fast** (LIFO)                   | Slower (managed by GC)              |
| üî• Overflow Risk   | **High** (deep recursion)              | Low (unless memory leak)            |
| üîÑ Lifetime         | Auto-freed after function returns      | Freed by Garbage Collector (GC)     |

---

### üîç Why Stack Overflows Easily

1. **Fixed small size**: The stack size per thread is limited (e.g., 1MB).
2. **Every recursive call** = one stack frame.
3. So deep recursion (like `fib(100000)`) = üí• stack overflow.

```java
// Stack will overflow if n is too large
int fib(int n) {
    if (n <= 1) return n;
    return fib(n - 1) + fib(n - 2);
}
```

---

### üì¶ Why Heap Doesn‚Äôt Overflow Easily

- **Much bigger** ‚Äî often **GBs** in size (configurable with `-Xmx`)
- Allocated for objects and data (like `new int[1000000]`)
- Managed by the **Garbage Collector (GC)**

```java
int[] dp = new int[1000000]; // Heap stores this, and it‚Äôs okay
```

---

### üß™ Example in Java

```bash
java -Xss512k MyStackProgram  # Stack size = 512 KB
java -Xmx2G MyHeapProgram     # Heap size = 2 GB
```

---

### üß† Conclusion

> ‚úÖ Heap = Big, Flexible, GC-managed ‚Üí Hard to overflow  
> ‚ùå Stack = Small, Per-thread, Recursive calls ‚Üí Easy to overflow

---

üîç **Why only 1‚Äì2 MB stack memory per thread?**

Because the **stack is fixed-size memory** allocated at thread creation, and keeping it small helps ensure:

---

### üß† 1. **Memory Efficiency**
- Every thread gets its own stack.
- If each thread got, say, 100 MB ‚Üí 1,000 threads = üí• 100 GB used.
- Keeping stack small (e.g. 1 MB) allows **thousands of threads** to run concurrently.

---

### üß∞ 2. **Faster Allocation**
- Stack memory is managed using **simple pointer arithmetic (LIFO)**.
- Small size ‚Üí quick allocation & deallocation ‚Üí **faster than heap**.

---

### üí• 3. **Avoid Crashes from Uncontrolled Recursion**
- Small size prevents infinite/very deep recursion from crashing the whole system.
- You get a **StackOverflowError** instead of system freeze.

---

### ‚öôÔ∏è 4. **OS/Architecture Limits**
- OS and CPU architecture have limits on per-thread memory regions.
- Exceeding this causes **segmentation faults** or undefined behavior.

---

### üõ†Ô∏è Bonus: You *Can* Change It (with care)

In Java:
```bash
java -Xss2m MyProgram   // 2 MB stack
```

In C/C++ (POSIX):
```cpp
pthread_attr_setstacksize(&attr, 2 * 1024 * 1024);  // Set to 2MB
```

But increasing it too much can reduce the number of threads your app can create.

---

### ‚úÖ Summary

> Stack is small **by design** to save memory, ensure performance, and prevent abuse.  
> If you need more space, prefer **tabulation or tail recursion optimization.**





